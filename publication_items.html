<div class="item mix cpaper ACL" data-year="2023">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/From-Dogwhistles-to-Bullhorns%3A-Unveiling-Coded-with-Mendelsohn-Bras/a5731b32060909bfc8848fa5f7e1e14ca3b53240" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models</h4>
                                                        <div class="pubauthor">Julia Mendelsohn, Ronan Le Bras, Yejin Choi, Maarten Sap</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">ACL</span> Association for Computational Linguistics, ACL 2023 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Dogwhistles are coded expressions that simultaneously convey one meaning to a broad audience and a second one, often hateful or provocative, to a narrow in-group; they are deployed to evade both political repercussions and algorithmic content moderation. For example, in the sentence 'we need to end the cosmopolitan experiment,' the word 'cosmopolitan' likely means 'worldly' to many, but secretly means 'Jewish' to a select few. We present the first large-scale computational investigation of dogwhistles. We develop a typology of dogwhistles, curate the largest-to-date glossary of over 300 dogwhistles with rich contextual information and examples, and analyze their usage in historical U.S. politicians' speeches. We then assess whether a large language model (GPT-3) can identify dogwhistles and their meanings, and find that GPT-3's performance varies widely across types of dogwhistles and targeted groups. Finally, we show that harmful content containing dogwhistles avoids toxicity detection, highlighting online risks of such coded language. This work sheds light on the theoretical and applied importance of dogwhistles in both NLP and computational social science, and provides resources for future research in modeling dogwhistles and mitigating their online harms.
            </p>
                                                    </div>
        </div><div class="item mix cpaper ACL" data-year="2023">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/I2D2%3A-Inductive-Knowledge-Distillation-with-and-Bhagavatula-Hwang/83562af413d730b9321efe8bea24058514ac940b" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation</h4>
                                                        <div class="pubauthor">Chandra Bhagavatula, Jena D. Hwang, Doug Downey, Ronan Le Bras, Ximing Lu, Keisuke Sakaguchi, Swabha Swayamdipta, Peter West, Yejin Choi</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">ACL</span> Association for Computational Linguistics, ACL 2023 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Commonsense capabilities of pre-trained language models dramatically improve with scale, leading many to believe that scale is the only winning recipe. But is it? Here, we investigate an alternative that a priori seems impossible: can smaller language models (e.g., GPT-2) win over models that are orders of magnitude larger and better (e.g., GPT-3), if powered with novel commonsense distillation algorithms? The key intellectual challenge is to design a learning algorithm that achieve a competitive level of commonsense acquisition, without relying on the benefits of scale. In particular, we study generative models of commonsense knowledge, focusing on the task of generating generics, statements of commonsense facts about everyday concepts, e.g., birds can fly. We introduce I2D2, a novel commonsense distillation framework that loosely follows the Symbolic Knowledge Distillation of West et al. but breaks the dependence on the extreme-scale teacher model with two innovations: (1) the novel adaptation of NeuroLogic Decoding to enhance the generation quality of the weak, off-the-shelf language models, and (2) self-imitation learning to iteratively learn from the model's own enhanced commonsense acquisition capabilities. Empirical results suggest that scale is not the only way, as novel algorithms can be a promising alternative. Moreover, our study leads to a new corpus of generics, Gen-A-tomic, that is the largest and highest quality available to date.
            </p>
                                                    </div>
        </div><div class="item mix cpaper ACL" data-year="2023">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/NLPositionality%3A-Characterizing-Design-Biases-of-Santy-Liang/a66ff335f5934fe7503a99d3eb3abed493994df1" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">NLPositionality: Characterizing Design Biases of Datasets and Models</h4>
                                                        <div class="pubauthor">Sebastin Santy, Jenny T. Liang, Ronan Le Bras, Katharina Reinecke, Maarten Sap</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">ACL</span> Association for Computational Linguistics, ACL 2023 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Design biases in NLP systems, such as performance differences for different populations, often stem from their creator’s positionality, i.e.,
views and lived experiences shaped by identity and background. Despite the prevalence and risks of design biases, they are hard to quantify because researcher, system, and dataset
positionality is often unobserved. We introduce NLPositionality, a framework for char-
acterizing design biases and quantifying the positionality of NLP datasets and models. Our
framework continuously collects annotations from a diverse pool of volunteer participants
on LabintheWild, and statistically quantifies alignment with dataset labels and model predictions. We apply NLPositionality to existing datasets and models for two tasks—social ac-
ceptability and hate speech detection. To date,
we have collected 16, 299 annotations in over
a year for 600 instances from 1, 096 annotators across 87 countries. We find that datasets
and models align predominantly with Western,
White, college-educated, and younger populations. Additionally, certain groups, such as non-
binary people and non-native English speakers,
are further marginalized by datasets and models as they rank least in alignment across all
tasks. Finally, we draw from prior literature
to discuss how researchers can examine their
own positionality and that of their datasets and
models, opening the door for more inclusive
NLP systems.
            </p>
                                                    </div>
        </div><div class="item mix cpaper Findings of ACL" data-year="2023">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/Modular-Transformers%3A-Compressing-Transformers-into-Zhou-Bras/1a7763f30c97b5b052af36bcfe478f64dcb97986" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Modular Transformers: Compressing Transformers into Modularized Layers for Flexible Efficient Inference</h4>
                                                        <div class="pubauthor">Wangchunshu Zhou, Ronan Le Bras, Yejin Choi</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">Findings of ACL</span> Association for Computational Linguistics, Findings of ACL 2023 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Pre-trained Transformer models like T5 and BART have advanced the state of the art on a wide range of
                text generation tasks. Compressing these models into smaller ones has become critically important for
                practical use. Common neural network compression techniques such as knowledge distillation or quantization
                are limited to static compression where the compression ratio is fixed. In this paper, we introduce
                Modular Transformers, a modularized encoder-decoder framework for flexible sequence-to-sequence model
                compression. Modular Transformers train modularized layers that have the same function of two or more
                consecutive layers in the original model via module replacing and knowledge distillation. After training,
                the modularized layers can be flexibly assembled into sequence-to-sequence models that meet different
                performance-efficiency trade-offs. Experimental results show that after a single training phase,
                by simply varying the assembling strategy, Modular Transformers can achieve flexible compression ratios
                from 1.1x to 6x with little to moderate relative performance drop.
            </p>
                                                    </div>
        </div><div class="item mix cpaper Findings of ACL" data-year="2023">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/Commonsense-Knowledge-Transfer-for-Pre-trained-Zhou-Bras/1c6c6a26d23e8343c6de06187818f0402c994812" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Commonsense Knowledge Transfer for Pre-trained Language Models</h4>
                                                        <div class="pubauthor">Wangchunshu Zhou, Ronan Le Bras, Yejin Choi</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">Findings of ACL</span> Association for Computational Linguistics, Findings of ACL 2023 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Despite serving as the foundation models for a wide range of NLP benchmarks, pre-trained language
                models have shown limited capabilities of acquiring implicit commonsense knowledge from
                self-supervision alone, compared to learning linguistic and factual knowledge that appear
                more explicitly in the surface patterns in text. In this work, we introduce commonsense
                knowledge transfer, a framework to transfer the commonsense knowledge stored in a neural
                commonsense knowledge model to a general-purpose pre-trained language model. It first
                exploits general texts to form queries for extracting commonsense knowledge from the neural
                commonsense knowledge model and then refines the language model with two self-supervised
                objectives: commonsense mask infilling and commonsense relation prediction, which align
                human language with the underlying commonsense knowledge. Empirical results show that our
                approach consistently improves the model's performance on downstream tasks that require
                commonsense reasoning. Moreover, we find that the improvement is more significant in the
                few-shot setting. This suggests that our approach helps language models better transfer
                to downstream tasks without extensive supervision by injecting commonsense knowledge into their parameters.
            </p>
                                                    </div>
        </div><div class="item mix cpaper EMNLP" data-year="2022">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/Neural-Theory-of-Mind-On-the-Limits-of-Social-in-Sap-Lebras/b4c16b0f26f9f5ad5e12f9bec3f1ad72eaa5491b" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs</h4>
                                                        <div class="pubauthor">Maarten Sap, Ronan Le Bras, Daniel Fried, Yejin Choi</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">EMNLP</span> Empirical Methods in Natural Language Processing, EMNLP 2022 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Social intelligence and Theory of Mind (T O M), i.e., the ability to reason about the different mental states, intents, and reactions of all people involved, allow humans to effectively navigate and understand everyday social interactions. As NLP systems are used in increasingly complex social situations, their ability to grasp social dynamics becomes crucial. In this work, we examine the open question of social intelligence and Theory of Mind in modern NLP systems from an empirical and theory-based perspective. We show that one of today’s largest language models (GPT-3; Brown et al., 2020) lacks this kind of social intelligence out-of-the box, using two tasks: S OCIAL IQ A (Sap et al., 2019b), which measures models’ ability to understand intents and reactions of participants of social interactions, and T O M I (Le et al., 2019), which measures whether models can infer mental states and realities of participants of situations. Our results show that models struggle substantially at these Theory of Mind tasks, with well-below-human accuracies of 55% and 60% on S OCIAL IQ A and T O M I , respectively. To conclude, we draw on theories from pragmatics to contextualize this shortcoming of large language models, by examining the limitations stemming from their data, neural archi-tecture, and training paradigms. Challenging the prevalent narrative that only scale is needed, we posit that person-centric NLP approaches might be more effective towards neural Theory of Mind.
            </p>
                                                    </div>
        </div><div class="item mix cpaper EMNLP" data-year="2022">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/Twist-Decoding%3A-Diverse-Generators-Guide-Each-Other-Kasai-Sakaguchi/ded0cd920c145ca0ae68acf306d54926cad5388c" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Twist Decoding: Diverse Generators Guide Each Other</h4>
                                                        <div class="pubauthor">Jungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Hao Peng, Ximing Lu, Dragomir Radev, Yejin Choi, Noah A. Smith</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">EMNLP</span> Empirical Methods in Natural Language Processing, EMNLP 2022 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Natural language generation technology has recently seen remarkable progress with large-scale training, and many natural language applications are now built upon a wide range of generation models. Combining diverse models may lead to further progress, but conven-tional ensembling (e.g., shallow fusion) requires that they share vocabulary/tokenization schemes. We introduce T WIST decoding, a simple and general inference algorithm that generates text while beneﬁting from diverse models. Our method does not assume the vocabulary, tokenization or even generation order is shared. Our extensive evaluations on machine translation and scientiﬁc paper summarization demonstrate that T WIST decoding substantially outperforms each model decoded in isolation over various scenarios, including cases where domain-speciﬁc and general-purpose models are both available. T WIST decoding also consistently outperforms the popular reranking heuristic where output candidates from one model is rescored by another. We hope that our work will encourage researchers and practitioners to examine generation models collectively, not just indepen-dently, and to seek out models with complementary strengths to the currently available models.
            </p>
                                                    </div>
        </div><div class="item mix cpaper EMNLP" data-year="2022">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/Maieutic-Prompting%3A-Logically-Consistent-Reasoning-Jung-Qin/50b0c6ee2b3d53ba5af69d6c00b5d60888a9026f" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations</h4>
                                                        <div class="pubauthor">Jaehun Jung, Lianhui Qin, S. Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, Yejin Choi</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">EMNLP</span> Empirical Methods in Natural Language Processing, EMNLP 2022 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Pre-trained language models (LMs) struggle with consistent reasoning; recently, prompting LMs to generate explanations that self-guide the inference has emerged as a promising direction to amend this. However, these approaches are fundamentally bounded by the correctness of explanations, which themselves are often noisy and inconsistent. In this work, we develop MAIEUTIC PROMPTING, which aims to infer a correct answer to a question even from the unreliable generations of LM. MAIEUTIC PROMPTING induces a tree of explanations abductively (e.g. X is true, because ... ) and recursively , then frames the inference as a satisﬁability problem over these explanations and their logical relations. We test MAIEUTIC PROMPTING for true/false QA on three challenging benchmarks that require complex commonsense reasoning. MAIEUTIC PROMPTING achieves up to 20% better accuracy than state-of-the-art prompting methods, and as a fully unsupervised approach, performs competitively with supervised models. We also show that MAIEUTIC PROMPTING improves robustness in inference while providing interpretable rationales.
            </p>
                                                    </div>
        </div><div class="item mix cpaper NAACL" data-year="2022">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/NeuroLogic-A*esque-Decoding%3A-Constrained-Text-with-Lu-Welleck/304cf21da84961469ac9f43405df187441832b61" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">NeuroLogic A*esque Decoding: Constrained Text Generation with Lookahead Heuristics</h4>
                                                        <div class="pubauthor">Ximing Lu, S. Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Ronan Le Bras, Lianhui Qin, Youngjae Yu, Rowan Zellers, Noah A. Smith, Yejin Choi</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">NAACL</span> North American Chapter of the Association for Computational Linguistics, NAACL 2022 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                The dominant paradigm for neural text generation is left-to-right decoding from autoregressive language models. Constrained or controllable generation under complex lexical constraints, however, requires foresight to plan ahead feasible future paths. Drawing inspiration from the A* search algorithm, we propose NEUROLOGIC AFesque,1 a decoding algorithm that incorporates heuristic estimates of future cost. We develop efficient lookahead heuristics that are efficient for large-scale language models, making our method a drop-in replacement for common techniques such as beam search and top-k sampling. To enable constrained generation, we build on NEUROLOGIC decoding (Lu et al., 2021), combining its flexibility in incorporating logical constraints with AFesque estimates of future constraint satisfaction. Our approach outperforms competitive baselines on five generation tasks, and achieves new state-of-the-art performance on table-totext generation, constrained machine translation, and keyword-constrained generation. The improvements are particularly notable on tasks that require complex constraint satisfaction or in few-shot or zero-shot settings. NEUROLOGIC AFesque illustrates the power of decoding for improving and enabling new capabilities of large-scale language models.
            </p>
                                                    </div>
        </div><div class="item mix cpaper NAACL" data-year="2022">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/Transparent-Human-Evaluation-for-Image-Captioning-Kasai-Sakaguchi/314a6ade391f591db23f59a932eb37f88ea38830" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Transparent Human Evaluation for Image Captioning</h4>
                                                        <div class="pubauthor">Jungo Kasai, Keisuke Sakaguchi, Lavinia Dunagan, Jacob Morrison, Ronan Le Bras, Yejin Choi, Noah A. Smith</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">NAACL</span> North American Chapter of the Association for Computational Linguistics, NAACL 2022 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                We establish a rubric-based human evaluation protocol for image captioning models. Our scoring rubrics and their definitions are carefully developed based on machineand humangenerated captions on the MSCOCO dataset. Each caption is evaluated along two main dimensions in a tradeoff (precision and recall) as well as other aspects that measure the text quality (fluency, conciseness, and inclusive language). Our evaluations demonstrate several critical problems of the current evaluation practice. Human-generated captions show substantially higher quality than machine-generated ones, especially in coverage of salient information (i.e., recall), while all automatic metrics say the opposite. Our rubric-based results reveal that CLIPScore, a recent metric that uses image features, better correlates with human judgments than conventional text-only metrics because it is more sensitive to recall. We hope that this work will promote a more transparent evaluation protocol for image captioning and its automatic metrics.
            </p>
                                                    </div>
        </div><div class="item mix cpaper NAACL" data-year="2022">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/Bidimensional-Leaderboards%3A-Generate-and-Evaluate-Kasai-Sakaguchi/718d30a66f032ebfa560070939349cedd4df28ad" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Bidimensional Leaderboards: Generate and Evaluate Language Hand in Hand</h4>
                                                        <div class="pubauthor">Jungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Lavinia Dunagan, Jacob Morrison, Alexander R. Fabbri, Yejin Choi, Noah A. Smith</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">NAACL</span> North American Chapter of the Association for Computational Linguistics, NAACL 2022 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Natural language processing researchers have identified limitations of evaluation methodology for generation tasks, with new questions raised about the validity of automatic metrics and of crowdworker judgments. Meanwhile, efforts to improve generation models tend to focus on simple n-gram overlap metrics (e.g., BLEU, ROUGE). We argue that new advances on models and metrics should each more directly benefit and inform the other. We therefore propose a generalization of leaderboards, bidimensional leaderboards (BILLBOARDs), that simultaneously tracks progress in language generation tasks and metrics for their evaluation. Unlike conventional unidimensional leaderboards that sort submitted systems by predetermined metrics, a BILLBOARD accepts both generators and evaluation metrics as competing entries. A BILLBOARD automatically creates an ensemble metric that selects and linearly combines a few metrics based on a global analysis across generators. Further, metrics are ranked based on their correlations with human judgments. We release four BILLBOARDs for machine translation, summarization, and image captioning.1 We demonstrate that a linear ensemble of a few diverse metrics sometimes substantially outperforms existing metrics in isolation. Our mixed-effects model analysis shows that most automatic metrics, especially the reference-based ones, overrate machine over human generation, demonstrating the importance of updating metrics as generation models become stronger (and perhaps more similar to humans) in the future.
            </p>
                                                    </div>
        </div><div class="item mix cpaper NAACL" data-year="2022">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/Symbolic-Knowledge-Distillation%3A-from-General-to-West-Bhagavatula/21d45b4923ad165fbb6612e08d06f9d786f9b4cc" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Symbolic Knowledge Distillation: from General Language Models to Commonsense Models</h4>
                                                        <div class="pubauthor">Peter West, Chandrasekhar Bhagavatula, Jack Hessel, Jena D. Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu, S. Welleck, Yejin Choi</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">NAACL</span> North American Chapter of the Association for Computational Linguistics, NAACL 2022 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                The common practice for training commonsense models has gone from–human–to– corpus–to–machine: humans author commonsense knowledge graphs in order to train commonsense models. In this work, we investigate an alternative, from–machine–to–corpus– to–machine: general language models author these commonsense knowledge graphs to train commonsense models. Our study leads to a new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge Distillation (Hinton et al., 2015), our approach uses larger models to teach smaller models. A key difference is that we distill knowledge symbolically–as text–in addition to the neural model. We also distill only one aspect–the commonsense of a general language model teacher, allowing the student to be a different type, a commonsense model. Altogether, we show that careful prompt engineering and a separately trained critic model allow us to selectively distill high-quality causal commonsense from GPT-3, a general language model. Empirical results demonstrate that, for the first time, a human-authored commonsense knowledge graph is surpassed by our automatically distilled variant in all three criteria: quantity, quality, and diversity. In addition, it results in a neural commonsense model that surpasses the teacher model’s commonsense capabilities despite its 100x smaller size. We apply this to the ATOMIC resource, and share our new symbolic knowledge graph and commonsense models1.
            </p>
                                                    </div>
        </div><div class="item mix cpaper ACL" data-year="2022">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/Generated-Knowledge-Prompting-for-Commonsense-Liu-Liu/12a763cb52f650710900790ca0bc43e5d5b88be6" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Generated Knowledge Prompting for Commonsense Reasoning</h4>
                                                        <div class="pubauthor">Jiachen Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi, Hannaneh Hajishirzi</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">ACL</span> Association for Computational Linguistics, ACL 2022 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Despite their ability to capture large amount of knowledge during pretraining, large-scale language models often benefit from incorporating external knowledge bases, especially on commonsense reasoning tasks. This motivates us to explore how we can best leverage knowledge elicited from language models themselves. We propose generating knowledge statements directly from a language model with a generic prompt format, then selecting the knowledge which maximizes prediction probability. Despite its simplicity, this approach improves performance of both off-theshelf and finetuned language models on four commonsense reasoning tasks, improving the state-of-the-art on numerical commonsense (NumerSense), general commonsense (CommonsenseQA 2.0), and scientific commonsense (QASC) benchmarks. Notably, we find that a model’s predictions can improve when using its own generated knowledge, demonstrating the importance of symbolic knowledge representation in neural reasoning processes.
            </p>
                                                    </div>
        </div><div class="item mix cpaper EMNLP" data-year="2021">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/CLIPScore%3A-A-Reference-free-Evaluation-Metric-for-Hessel-Holtzman/727f49d1f5f862441f56410489e86ae2d3ecd551" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">CLIPScore: A Reference-free Evaluation Metric for Image Captioning</h4>
                                                        <div class="pubauthor">Jack Hessel, Ariel Holtzman, Maxwell Forbes, Ronan Le Bras, Yejin Choi</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">EMNLP</span> Empirical Methods in Natural Language Processing, EMNLP 2021 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Image captioning has conventionally relied on reference-based automatic evaluations, where machine captions are compared against captions written by humans. This is in stark contrast to the reference-free manner in which humans assess caption quality. In this paper, we report the surprising empirical finding that CLIP (Radford et al., 2021), a cross-modal model pretrained on 400M image+caption pairs from the web, can be used for robust automatic evaluation of image captioning without the need for references. Experiments spanning several corpora demonstrate that our new reference-free metric, CLIPScore, achieves the highest correlation with human judgements, outperforming existing reference-based metrics like CIDEr and SPICE. Information gain experiments demonstrate that CLIPScore, with its tight focus on image–text compatibility, is complementary to existing reference-based metrics that emphasize text–text similarities. Thus, we also present a reference-augmented version, RefCLIPScore, which achieves even higher correlation. Beyond literal description tasks, several case studies reveal domains where CLIPScore performs well (clip-art images, alt-text rating), but also where it is relatively weaker vs reference-based metrics, e.g., news captions that require richer contextual knowledge.
            </p>
                                                    </div>
        </div><div class="item mix cpaper EMNLP" data-year="2021">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/Moral-Stories%3A-Situated-Reasoning-about-Norms%2C-and-Emelin-Bras/22e5af80e9fd64b817efd2ec50c23b3b454f5a0d" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Moral Stories: Situated Reasoning about Norms, Intents, Actions, and their Consequences</h4>
                                                        <div class="pubauthor">Denis Emelin, Ronan Le Bras, Jena D. Hwang, Maxwell Forbes, Yejin Choi</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">EMNLP</span> Empirical Methods in Natural Language Processing, EMNLP 2021 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                In social settings, much of human behavior is governed by unspoken rules of conduct. For artificial systems to be fully integrated into social environments, adherence to such norms is a central prerequisite. We investigate whether contemporary NLG models can function as behavioral priors for systems deployed in social settings by generating action hypotheses that achieve predefined goals under moral constraints. Moreover, we examine if models can anticipate likely consequences of (im)moral actions, or explain why certain actions are preferable by generating relevant norms. For this purpose, we introduce Moral Stories, a crowd-sourced dataset of structured, branching narratives for the study of grounded, goaloriented social reasoning. Finally, we propose decoding strategies that effectively combine multiple expert models to significantly improve the quality of generated actions, consequences, and norms compared to strong baselines, e.g. though abductive reasoning.
            </p>
                                                    </div>
        </div><div class="item mix cpaper Findings of EMNLP" data-year="2021">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/proScript%3A-Partially-Ordered-Scripts-Generation-via-Sakaguchi-Bhagavatula/53e161d4434576355fc5f63fe56afd8e135174b2" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">proScript: Partially Ordered Scripts Generation via Pre-trained Language Models</h4>
                                                        <div class="pubauthor">Keisuke Sakaguchi, Chandra Bhagavatula, Ronan Le Bras, Niket Tandon, Peter Clark, Yejin Choi</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">Findings of EMNLP</span> Empirical Methods in Natural Language Processing, Findings of EMNLP 2021 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Scripts standardized event sequences describing typical everyday activities have been shown to help understand narratives by providing expectations, resolving ambiguity, and filling in unstated information. However, to date they have proved hard to author or extract from text. In this work, we demonstrate for the first time that pre-trained neural language models (LMs) can be be finetuned to generate high-quality scripts, at varying levels of granularity, for a wide range of everyday scenarios (e.g., bake a cake). To do this, we collected a large (6.4k), crowdsourced partially ordered scripts (named proScript), which is substantially larger than prior datasets, and developed models that generate scripts with combining language generation and structure prediction. We define two complementary tasks: (i) edge prediction: given a scenario and unordered events, organize the events into a valid (possibly partial-order) script, and (ii) script generation: given only a scenario, generate events and organize them into a (possibly partial-order) script. Our experiments show that our models perform well (e.g., F1=75.7 on task (i)), illustrating a new approach to overcoming previous barriers to script collection. We also show that there is still significant room for improvement toward human level performance. Together, our tasks, dataset, and models offer a new research direction for learning script knowledge.
            </p>
                                                    </div>
        </div><div class="item mix cpaper AKBC" data-year="2021">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/Analyzing-Commonsense-Emergence-in-Few-shot-Models/aa399f7f09a3e2b9a44ba89203fad77a52fa1180" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Analyzing Commonsense Emergence in Few-shot Knowledge Models</h4>
                                                        <div class="pubauthor">Jeff Da, Ronan Le Bras, Ximing Lu, Yejin Choi, Antoine Bosselut</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">AKBC</span> Automated Knowledge Base Construction, AKBC 2021 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Recently, commonsense knowledge models — pretrained language models (LM) finetuned on knowledge graph (KG) tuples — showed that considerable amounts of commonsense knowledge can be encoded in the parameters of large language models [Bosselut et al., 2019]. However, as parallel studies show that LMs are poor hypothesizers of declarative commonsense relationships [Petroni et al., 2019] on their own, it remains unclear whether this knowledge is learned during pretraining or from fine-tuning on KG examples. To investigate this question, we train commonsense knowledge models in few-shot settings to study the emergence of their commonsense representation abilities. Our results show that commonsense knowledge models can rapidly adapt from limited examples, indicating that KG fine-tuning serves to learn an interface to encoded knowledge learned during pretraining. Importantly, our analysis of absolute, angular, and distributional parameter changes during few-shot fine-tuning provides novel insights into how this interface is learned.
            </p>
                                                    </div>
        </div><div class="item mix cpaper NeurIPS Datasets and Benchmarks Track" data-year="2021">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/NaturalProofs%3A-Mathematical-Theorem-Proving-in-Welleck-Liu/4cc1fb128fa3abf6f90d567744767e8fd6315e1d" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">NaturalProofs: Mathematical Theorem Proving in Natural Language</h4>
                                                        <div class="pubauthor">Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi, Yejin Choi, Kyunghyun Cho</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">NeurIPS Datasets and Benchmarks Track</span> Conference on Neural Information Processing Systems, NeurIPS Datasets and Benchmarks Track 2021 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Understanding and creating mathematics using natural mathematical language – the mixture of symbolic and natural language used by humans – is a challenging and important problem for driving progress in machine learning. As a step in this direction, we develop NATURALPROOFS, a largescale dataset of mathematical statements and their proofs, written in natural mathematical language. Using NATURALPROOFS, we propose a mathematical reference retrieval task that tests a system’s ability to determine the key results that appear in a proof. Large-scale sequence models excel at this task compared to classical information retrieval techniques, and benefit from language pretraining, yet their performance leaves substantial room for improvement. NATURALPROOFS opens many possibilities for future research on challenging mathematical tasks.
            </p>
                                                    </div>
        </div><div class="item mix cpaper NeurIPS Datasets and Benchmarks Track" data-year="2021">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/CommonsenseQA-2.0%3A-Exposing-the-Limits-of-AI-Talmor-Yoran/d65a064eb837f838faf6ff67781b62450b92b159" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">CommonsenseQA 2.0: Exposing the Limits of AI through Gamification</h4>
                                                        <div class="pubauthor">Alon Talmor, Ori Yoran, Ronan Le Bras, Chandra Bhagavatula, Yoav Goldberg, Yejin Choi, Jonathan Berant</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">NeurIPS Datasets and Benchmarks Track</span> Conference on Neural Information Processing Systems, NeurIPS Datasets and Benchmarks Track 2021 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Constructing benchmarks that test the abilities of modern natural language understanding models is difficult - pre-trained language models exploit artifacts in benchmarks to achieve human parity, but still fail on adversarial examples and make errors that demonstrate a lack of common sense. In this work, we propose gamification as a framework for data construction.
The goal of players in the game is to compose questions that mislead a rival AI while using specific phrases for extra points. The game environment leads to enhanced user engagement and simultaneously gives the game designer control over the collected data, allowing us to collect high-quality data at scale. Using our method we create CommonsenseQA 2.0, which includes 14,343 yes/no questions, and demonstrate its difficulty for models that are orders-of-magnitude larger than the AI used in the game itself.
Our best baseline, the T5-based Unicorn with 11B parameters achieves an accuracy of 70.2%, substantially higher than GPT-3 (52.9%) in a few-shot inference setup.  Both score well below human performance which is at 94.1%.
</p>
                                                    </div>
        </div><div class="item mix cpaper NAACL" data-year="2021">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/NeuroLogic-Decoding%3A-(Un)supervised-Neural-Text-Lu-West/2c5bf29079cd958a2bef150077a02a1deb300652" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">NeuroLogic Decoding: (Un)supervised Neural Text Generation with Predicate Logic Constraints</h4>
                                                        <div class="pubauthor">Ximing Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">NAACL</span> North American Chapter of the Association for Computational Linguistics, NAACL 2021 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Conditional text generation often requires lexical constraints, i.e., which words should or shouldn't be included in the output text. While the dominant recipe for conditional text generation has been large-scale pretrained language models that are finetuned on the task-specific training data, such models do not learn to follow the underlying constraints reliably, even when supervised with large amounts of task-specific examples. We propose NeuroLogic Decoding, a simple yet effective algorithm that enables neural language models -- supervised or not -- to generate fluent text while satisfying complex lexical constraints. Our approach is powerful yet efficient. It handles any set of lexical constraints that is expressible under predicate logic, while its asymptotic runtime is equivalent to conventional beam search. Empirical results on four benchmarks show that NeuroLogic Decoding outperforms previous approaches, including algorithms that handle a subset of our constraints. Moreover, we find that unsupervised models with NeuroLogic Decoding often outperform supervised models with conventional decoding, even when the latter is based on considerably larger networks. Our results suggest the limit of large-scale neural networks for fine-grained controllable generation and the promise of inference-time algorithms.
            </p>
                                                    </div>
        </div><div class="item mix cpaper AAAI" data-year="2021">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://api.semanticscholar.org/e39503e01ebb108c6773948a24ca798cd444eb62" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">COMET-ATOMIC 2020: On Symbolic and Neural Commonsense Knowledge Graphs</h4>
                                                        <div class="pubauthor">Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke Sakaguchi, Antoine Bosselut, Yejin Choi</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">AAAI</span> Conference on Artificial Intelligence, AAAI 2021 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Recent years have brought about a renewed interest in commonsense representation and reasoning in the field of natural language understanding. The development of new commonsense knowledge graphs (CSKG) has been central to these advances as their diverse facts can be used and referenced by machine learning models for tackling new and challenging tasks. At the same time, there remain questions about the quality and coverage of these resources due to the massive scale required to comprehensively encompass general commonsense knowledge.
In this work, we posit that manually constructed CSKGs will never achieve the coverage necessary to be applicable in all situations encountered by NLP agents. Therefore, we propose a new evaluation framework for testing the utility of KGs based on how effectively implicit knowledge representations can be learned from them.
With this new goal, we propose ATOMIC 2020, a new CSKG of general-purpose commonsense knowledge containing knowledge that is not readily available in pretrained language models. We evaluate its properties in comparison with other leading CSKGs, performing the first large-scale pairwise study of commonsense knowledge resources. Next, we show that ATOMIC 2020 is better suited for training knowledge models that can generate accurate, representative knowledge for new, unseen entities and events. Finally, through human evaluation, we show that the few-shot performance of GPT-3 (175B parameters), while impressive, remains ~12 absolute points lower than a BART-based knowledge model trained on ATOMIC 2020 despite using over 430x fewer parameters.
            </p>
                                                    </div>
        </div><div class="item mix cpaper AAAI" data-year="2021">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New Multitask Benchmark</h4>
                                                        <div class="pubauthor">Nicholas Lourie, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">AAAI</span> Conference on Artificial Intelligence, AAAI 2021 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                As AI systems become an increasing part of people's everyday lives, it becomes ever more important that they understand people's ethical norms. Motivated by descriptive ethics, a field of study that focuses on people's descriptive judgments rather than theoretical prescriptions on morality, we investigate a novel, data-driven approach to machine ethics.
We introduce SCRUPLES, the first large-scale dataset with 625,000 ethical judgments over 32,000 real-life anecdotes. Each anecdote recounts a complex ethical situation, often posing moral dilemmas, paired with a distribution of judgments contributed by the community members. Our dataset presents a major challenge to state-of-the-art neural language models, leaving significant room for improvement. However, when presented with simplified moral situations, the results are considerably more promising, suggesting that neural models can effectively learn simpler ethical building blocks.
A key take-away of our empirical analysis is that norms are not always clean-cut; many situations are naturally divisive. We present a new method to estimate the best possible performance on such tasks with inherently diverse label distributions, and explore likelihood functions that separate intrinsic from model uncertainty.
                   </p>
                                                    </div>
        </div><div class="item mix cpaper AAAI" data-year="2021">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://api.semanticscholar.org/ba3c0aa5c9057140e08872e908efe48791af3083" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Scruples: A Corpus of Community Ethical Judgments on 32, 000 Real-Life Anecdotes</h4>
                                                        <div class="pubauthor">Nicholas Lourie, Ronan Le Bras, Yejin Choi</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">AAAI</span> Conference on Artificial Intelligence, AAAI 2021 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                As AI systems become an increasing part of people's everyday lives, it becomes ever more important that they understand people's ethical norms. Motivated by descriptive ethics, a field of study that focuses on people's descriptive judgments rather than theoretical prescriptions on morality, we investigate a novel, data-driven approach to machine ethics.
We introduce Scruples, the first large-scale dataset with 625,000 ethical judgments over 32,000 real-life anecdotes. Each anecdote recounts a complex ethical situation, often posing moral dilemmas, paired with a distribution of judgments contributed by the community members. Our dataset presents a major challenge to state-of-the-art neural language models, leaving significant room for improvement. However, when presented with simplified moral situations, the results are considerably more promising, suggesting that neural models can effectively learn simpler ethical building blocks.
A key take-away of our empirical analysis is that norms are not always clean-cut; many situations are naturally divisive. We present a new method to estimate the best possible performance on such tasks with inherently diverse label distributions, and explore likelihood functions that separate intrinsic from model uncertainty.
            </p>
                                                    </div>
        </div><div class="item mix cpaper AAAI" data-year="2021">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://api.semanticscholar.org/e2ecce8134a736444065e28a5c12344245b13f7d" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Dynamic Neuro-Symbolic Knowledge Graph Construction for Zero-shot Commonsense Question Answering</h4>
                                                        <div class="pubauthor">Antoine Bosselut, Ronan Le Bras, Yejin Choi</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">AAAI</span> Conference on Artificial Intelligence, AAAI 2021 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Understanding narratives requires reasoning about implicit world knowledge related to the causes, effects, and states of situations described in text. At the core of this challenge is how to access contextually relevant knowledge on demand and reason over it. In this paper, we present initial studies toward zero-shot commonsense question answering by formulating the task as inference over dynamically generated commonsense knowledge graphs. In contrast to previous studies for knowledge integration that rely on retrieval of existing knowledge from static knowledge graphs, our study requires commonsense knowledge integration where contextually relevant knowledge is often not present in existing knowledge bases. Therefore, we present a novel approach that generates contextually-relevant symbolic knowledge structures on demand using generative neural commonsense knowledge models. Empirical results on two datasets demonstrate the efficacy of our neuro-symbolic approach for dynamically constructing knowledge graphs for reasoning. Our approach achieves significant performance boosts over pretrained language models and vanilla knowledge models, all while providing interpretable reasoning paths for its predictions.
            </p>
                                                    </div>
        </div><div class="item mix cpaper AAAI" data-year="2021">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://api.semanticscholar.org/5dfc43bb697acf5eacf8b8a05d78dba8beb0dd42" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Paragraph-Level Commonsense Transformers with Recurrent Memory</h4>
                                                        <div class="pubauthor">Saadia Gabriel, Chandra Bhagavatula, Vered Shwartz, Ronan Le Bras, Max Forbes, Yejin Choi</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">AAAI</span> Conference on Artificial Intelligence, AAAI 2021 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Human understanding of narrative texts requires making commonsense inferences beyond what is stated in the text explicitly. A recent model, COMeT, can generate such inferences along several dimensions such as pre- and post-conditions, motivations, and mental-states of the participants. However, COMeT was trained on short phrases, and is therefore discourse-agnostic. When presented with each sentence of a multi-sentence narrative, it might generate inferences that are inconsistent with the rest of the narrative.
We present the task of discourse-aware commonsense inference. Given a sentence within a narrative, the goal is to generate commonsense inferences along predefined dimensions, while maintaining coherence with the rest of the narrative. Such large-scale paragraph-level annotation is hard to get and costly, so we use available sentence-level annotations to efficiently and automatically construct a distantly supervised corpus.
Using this corpus, we train PARA-COMeT, a discourse-aware model that incorporates paragraph-level information to generate coherent commonsense inferences from narratives. PARA-COMeT captures both semantic knowledge pertaining to prior world knowledge, and episodic knowledge involving how current events relate to prior and future events in a narrative. Our results confirm that PARA-COMeT outperforms the sentence-level baselines, particularly in generating inferences that are both coherent and novel.
            </p>
                                                    </div>
        </div><div class="item mix cpaper EMNLP" data-year="2020">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/Unsupervised-Commonsense-Question-Answering-with-Shwartz-West/a45b430f057a48b2d4c31c9278248c2b43780bf8" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Unsupervised Commonsense Question Answering with Self-Talk</h4>
                                                        <div class="pubauthor">Vered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">EMNLP</span> Empirical Methods in Natural Language Processing, EMNLP 2020 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Natural language understanding involves reading between the lines with implicit background knowledge. Current systems either rely on pre-trained language models as the sole implicit source of world knowledge, or resort to external knowledge bases (KBs) to incorporate additional relevant knowledge. We propose an unsupervised framework based on \emph{self-talk} as a novel alternative to multiple-choice commonsense tasks. Inspired by inquiry-based discovery learning (Bruner, 1961), our approach inquires language models with a number of information seeking questions such as "$\textit{what is the definition of ...}$" to discover additional background knowledge. Empirical results demonstrate that the self-talk procedure substantially improves the performance of zero-shot language model baselines on four out of six commonsense benchmarks, and competes with models that obtain knowledge from external KBs. While our approach improves performance on several benchmarks, the self-talk induced knowledge even when leading to correct answers is not always seen as useful by human judges, raising interesting questions about the inner-workings of pre-trained language models for commonsense reasoning.
            </p>
                                                    </div>
        </div><div class="item mix cpaper EMNLP" data-year="2020">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/Back-to-the-Future%3A-Unsupervised-Backprop-based-for-Qin-Shwartz/14c454c27dffd655cea839a0684a2d855117cd58" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Back to the Future: Unsupervised Backprop-based Decoding for Counterfactual and Abductive Commonsense Reasoning</h4>
                                                        <div class="pubauthor">Lianhui Qin, Vered Shwartz, P. West, Chandra Bhagavatula, Jena D. Hwang, Ronan Le Bras, Antoine Bosselut, Yejin Choi</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">EMNLP</span> Empirical Methods in Natural Language Processing, EMNLP 2020 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
               Abductive and counterfactual reasoning, core abilities of everyday human cognition, require reasoning about what might have happened at time t, while conditioning on multiple contexts from the relative past and future. However, simultaneous incorporation of past and future contexts using generative language models (LMs) can be challenging, as they are trained either to condition only on the past context or to perform narrowly scoped text-infilling. In this paper, we propose DELOREAN, a new unsupervised decoding algorithm that can flexibly incorporate both the past and future contexts using only off-the-shelf, left-to-right language models and no supervision. The key intuition of our algorithm is incorporating the future through back-propagation, during which, we only update the internal representation of the output while fixing the model parameters. By alternating between forward and backward propagation, DELOREAN can decode the output representation that reflects both the left and right contexts. We demonstrate that our approach is general and applicable to two nonmonotonic reasoning tasks: abductive text generation and counterfactual story revision, where DELOREAN outperforms a range of unsupervised and some supervised methods, based on automatic and human evaluation.
            </p>
                                                    </div>
        </div><div class="item mix cpaper Findings of EMNLP" data-year="2020">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/Thinking-Like-a-Skeptic%3A-Defeasible-Inference-in-Rudinger-Shwartz/47e799f83b0850f3d036a2e3a66bb337661b7e68" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Thinking Like a Skeptic: Defeasible Inference in Natural Language</h4>
                                                        <div class="pubauthor">Rachel Rudinger, Vered Shwartz, Jena D. Hwang, Chandra Bhagavatula, Maxwell Forbes, Ronan Le Bras, Noah A. Smith, Yejin Choi</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">Findings of EMNLP</span> Empirical Methods in Natural Language Processing, Findings of EMNLP 2020 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Defeasible inference is a mode of reasoning in which an inference (X is a bird, therefore X flies) may be weakened or overturned in light of new evidence (X is a penguin). Though long recognized in classical AI and philosophy, defeasible inference has not been extensively studied in the context of contemporary data-driven research on natural language inference and commonsense reasoning. We introduce Defeasible NLI (abbreviated δ-NLI), a dataset for defeasible inference in natural language. δ-NLI contains extensions to three existing inference datasets covering diverse modes of reasoning: common sense, natural language inference, and social norms. From δ-NLI, we develop both a classification and generation task for defeasible inference, and demonstrate that the generation task is much more challenging. Despite lagging human performance, however, generative models trained on this data are capable of writing sentences that weaken or strengthen a specified inference up to 68% of the time.
            </p>
                                                    </div>
        </div><div class="item mix cpaper Findings of EMNLP" data-year="2020">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/Natural-Language-Rationales-with-Full-Stack-Visual-Marasovi%C4%87-Bhagavatula/c9940a17504a3b83bd1e9d613b095ddb204d2ad0" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Natural Language Rationales with Full-Stack Visual Reasoning: From Pixels to Semantic Frames to Commonsense Graphs</h4>
                                                        <div class="pubauthor">Ana Marasović, Chandra Bhagavatula, J. Park, Ronan Le Bras, Noah A. Smith, Yejin Choi</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">Findings of EMNLP</span> Empirical Methods in Natural Language Processing, Findings of EMNLP 2020 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Natural language rationales could provide intuitive, higher-level explanations that are easily understandable by humans, complementing the more broadly studied lower-level explanations based on gradients or attention weights. We present the first study focused on generating natural language rationales across several complex visual reasoning tasks: visual commonsense reasoning, visual-textual entailment, and visual question answering. The key challenge of accurate rationalization is comprehensive image understanding at all levels: not just their explicit content at the pixel level, but their contextual contents at the semantic and pragmatic levels. We present RATIONALEVT TRANSFORMER, an integrated model that learns to generate free-text rationales by combining pretrained language models with object recognition, grounded visual semantic frames, and visual commonsense graphs. Our experiments show that the base pretrained language model benefits from visual adaptation and that freetext rationalization is a promising research direction to complement model interpretability for complex visual-textual reasoning tasks.
            </p>
                                                    </div>
        </div><div class="item mix cpaper Findings of EMNLP" data-year="2020">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/Generative-Data-Augmentation-for-Commonsense-Yang-Malaviya/dd6f3b6d92ae9448a2000d9690b921f545f00256" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Generative Data Augmentation for Commonsense Reasoning</h4>
                                                        <div class="pubauthor">Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan Le Bras, J. Wang, Chandra Bhagavatula, Yejin Choi, Doug Downey</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">Findings of EMNLP</span> Empirical Methods in Natural Language Processing, Findings of EMNLP 2020 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Recent advances in commonsense reasoning depend on large-scale human-annotated training data to achieve peak performance. However, manual curation of training examples is expensive and has been shown to introduce annotation artifacts that neural models can readily exploit and overfit on. We investigate G-DAUG, a novel generative data augmentation method that aims to achieve more accurate and robust learning in the low-resource setting. Our approach generates synthetic examples using pretrained language models, and selects the most informative and diverse set of examples for data augmentation. In experiments with multiple commonsense reasoning benchmarks, G-DAUG consistently outperforms existing data augmentation methods based on back-translation, and establishes a new state-of-the-art on WinoGrande, CODAH, and CommonsenseQA. Further, in addition to improvements in in-distribution accuracy, G-DAUG-augmented training also enhances out-of-distribution generalization, showing greater robustness against adversarial or perturbed examples. Our analysis demonstrates that G-DAUG produces a diverse set of fluent training examples, and that its selection and training approaches are important for performance. Our findings encourage future research toward generative data augmentation to enhance both in-distribution learning and out-of-distribution generalization.
            </p>
                                                    </div>
        </div><div class="item mix cpaper ICML" data-year="2020">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/Adversarial-Filters-of-Dataset-Biases-Bras-Swayamdipta/22d834f7983fbd7cf2418978571f23efcd224bd9" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Adversarial Filters of Dataset Biases</h4>
                                                        <div class="pubauthor">Ronan Le Bras, Swabha Swayamdipta, Chandra Bhagavatula, Rowan Zellers, Matthew E. Peters, Ashish Sabharwal, Yejin Choi</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">ICML</span> International Conference on Machine Learning, ICML 2020 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Large neural models have demonstrated human-level performance on language and vision benchmarks such as ImageNet and Stanford Natural Language Inference (SNLI). Yet, their performance degrades considerably when tested on adversarial or out-of-distribution samples. This raises the question of whether these models have learned to solve a dataset rather than the underlying task by overfitting on spurious dataset biases. We investigate one recently proposed approach, AFLite, which adversarially filters such dataset biases, as a means to mitigate the prevalent overestimation of machine performance. We provide a theoretical understanding for AFLite, by situating it in the generalized framework for optimum bias reduction. Our experiments show that as a result of the substantial reduction of these biases, models trained on the filtered datasets yield better generalization to out-of-distribution tasks, especially when the benchmarks used for training are over-populated with biased samples. We show that AFLite is broadly applicable to a variety of both real and synthetic datasets for reduction of measurable dataset biases and provide extensive supporting analyses. Finally, filtering results in a large drop in model performance (e.g., from 92% to 63% for SNLI), while human performance still remains high. Our work thus shows that such filtered datasets can pose new research challenges for robust generalization by serving as upgraded benchmarks.
            </p>
                                                    </div>
        </div><div class="item mix jpaper J Econ Behav Organ." data-year="2020">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.dropbox.com/s/wziu46961t34crn/Jensenetal_RR_022020.pdf?dl=0" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Conspicuous Monitoring and Remote Work</h4>
                                                        <div class="pubauthor">Nathaniel Jensen, Elizabeth Lyons, Eddy Chebelyon, Ronan Le Bras, Carla Gomes</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">J Econ Behav Organ.</span> The Journal of Economic Behavior and Organization, J Econ Behav Organ. 2020 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Credible monitoring of remote workers presents unique challenges that may reduce the benefits
of formal organization for their management. We consider whether increasing the salience of
monitor productivity without changing incentive contracts or monitoring technology leads to
changes in remote worker performance. Results from a field experiment run among multidimensional task workers in Kenya demonstrate that increasing the visibility of monitor activity
improves performance on task dimensions not being directly paid for. Our evidence is consistent
with the importance of conspicuous monitoring when managers and workers are not co-located.
            </p>
                                                    </div>
        </div><div class="item mix cpaper ICLR" data-year="2020">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/a550f576ff20b8cce98f3ddad0043d3783fbc9b4" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Abductive Commonsense Reasoning</h4>
                                                        <div class="pubauthor">Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Scott Yih, Yejin Choi</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">ICLR</span> International Conference on Learning Representations, ICLR 2020 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Abductive reasoning is inference to the most plausible explanation. For example, if
                Jenny finds her house in a mess when she returns from work, and remembers that she
                left a window open, she can hypothesize that a thief broke into her house and caused
                the mess, as the most plausible explanation. While abduction has long been
                considered to be at the core of how people interpret and read between the lines in
                natural language (Hobbs et al. (1988)), there has been relatively little NLP
                research in support of abductive natural language inference. We present the first
                study that investigates the viability of language-based abductive reasoning. We
                conceptualize a new task of Abductive NLI and introduce a challenge dataset, ART,
                that consists of over 20k commonsense narrative contexts and 200k explanations,
                formulated as multiple choice questions for easy automatic evaluation. We establish
                comprehensive baseline performance on this task based on state-of-the-art NLI and
                language models, which leads to 68.9% accuracy, well below human performance
                (91.4%). Our analysis leads to new insights into the types of reasoning that deep
                pre-trained language models fail to perform -- despite their strong performance on
                the related but fundamentally different task of entailment NLI -- pointing to
                interesting avenues for future research.
            </p>
                                                    </div>
        </div><div class="item mix cpaper AAAI" data-year="2020">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/8f7133b2e3851b09d659b91e8faa761ec206413f" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">WinoGrande: An Adversarial Winograd Schema Challenge at Scale</h4>
                                                        <div class="pubauthor">Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">AAAI</span> Conference on Artificial Intelligence, AAAI 2020 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                The Winograd Schema Challenge (WSC), proposed by Levesque et al. (2011) as an
                alternative to the Turing Test, was originally designed as a pronoun resolution
                problem that cannot be solved based on statistical patterns in large text corpora.
                However, recent studies suggest that current WSC datasets, even when composed
                carefully by experts, are still prone to such biases that statistical methods can
                exploit. We introduce WINOGRANDE, a new collection of WSC problems that are
                adversarially constructed to be robust against spurious statistical biases. While
                the original WSC dataset provided only 273 instances, WINOGRANDE includes 43,985
                instances, half of which are determined as adversarial. Key to our approach is a
                novel adversarial filtering algorithm AFLITE for systematic bias reduction, combined
                with a careful crowdsourcing design. Despite the significant increase in training
                data, the performance of existing state-of-the-art methods remains modest (61.6%)
                and contrasts with high human performance (90.8%) for the binary questions. In
                addition, WINOGRANDE allows us to use transfer learning for achieving new
                state-of-the-art results on the original WSC and related datasets. Finally, we
                discuss how biases lead to overestimating the true capabilities of machine
                commonsense.
            </p>
                                                    </div>
        </div><div class="item mix cpaper AAAI" data-year="2020">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/04f4e55e14150b7c48b0287ba77c7443df76ed45" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">PIQA: Reasoning about Physical Commonsense in Natural Language</h4>
                                                        <div class="pubauthor">Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, Yejin Choi</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">AAAI</span> Conference on Artificial Intelligence, AAAI 2020 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                To apply eyeshadow without a brush, should I use a cotton swab or a toothpick?
                Questions requiring this kind of physical commonsense pose a challenge to today’s
                natural language understanding systems. While recent pretrained models (such as
                BERT) have made progress on question answering over more abstract domains – such as
                news articles and encyclopedia entries, where text is plentiful – in more physical
                domains, text is inherently limited due to reporting bias. Can AI systems learn to
                reliably answer physical commonsense questions without experiencing the physical
                world? In this paper, we introduce the task of physical commonsense reasoning and a
                corresponding benchmark dataset Physical Interaction: Question Answering or PIQA .
                Though humans find the dataset easy (95% accuracy), large pretrained models struggle
                (∼77%). We provide analysis about the dimensions of knowledge that existing models
                lack, which offers significant opportunities for future research.
            </p>
                                                    </div>
        </div><div class="item mix cpaper EMNLP" data-year="2019">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/421cb75cc91e8e5683d41ee6a918121aedf6d24d" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Social IQa: Commonsense Reasoning about Social Interactions</h4>
                                                        <div class="pubauthor">Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, Yejin Choi</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">EMNLP</span> Empirical Methods in Natural Language Processing, EMNLP 2019 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                We introduce Social IQa, the first largescale benchmark for commonsense reasoning
                about social situations. Social IQa contains 38,000 multiple choice questions for
                probing emotional and social intelligence in a variety of everyday situations (e.g.,
                Q: "Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did
                Jordan do this?" A: "Make sure no one else could hear"). Through crowdsourcing, we
                collect commonsense questions along with correct and incorrect answers about social
                interactions, using a new framework that mitigates stylistic artifacts in incorrect
                answers by asking workers to provide the right answer to a different but related
                question. Empirical results show that our benchmark is challenging for existing
                question-answering models based on pretrained language models, compared to human
                performance (>20% gap). Notably, we further establish Social IQa as a resource
                for transfer learning of commonsense knowledge, achieving state-of-the-art
                performance on multiple commonsense reasoning tasks (Winograd Schemas, COPA).
            </p>
                                                    </div>
        </div><div class="item mix cpaper EMNLP" data-year="2019">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://api.semanticscholar.org/arXiv:1909.00277" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning</h4>
                                                        <div class="pubauthor">Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">EMNLP</span> Empirical Methods in Natural Language Processing, EMNLP 2019 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Understanding narratives requires reading between the lines, which in turn, requires
                interpreting the likely causes and effects of events, even when they are not
                mentioned explicitly. In this paper, we introduce Cosmos QA, a large-scale dataset
                of 35,600 problems that require commonsense-based reading comprehension, formulated
                as multiple-choice questions. In stark contrast to most existing reading
                comprehension datasets where the questions focus on factual and literal
                understanding of the context paragraph, our dataset focuses on reading between the
                lines over a diverse collection of people's everyday narratives, asking such
                questions as "what might be the possible reason of ...?", or "what would have
                happened if ..." that require reasoning beyond the exact text spans in the context.
                To establish baseline performances on Cosmos QA, we experiment with several
                state-of-the-art neural architectures for reading comprehension, and also propose a
                new architecture that improves over the competitive baselines. Experimental results
                demonstrate a significant gap between machine (68.4%) and human performance (94%),
                pointing to avenues for future research on commonsense machine comprehension.
                Dataset, code and leaderboard is publicly available at this
                .
            </p>
                                                    </div>
        </div><div class="item mix cpaper SemEval-NAACL" data-year="2019">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/655acd2a4dd59f541e6542594139e1b332543305" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">SemEval 2019 Task 10: Math Question Answering</h4>
                                                        <div class="pubauthor">Mark Hopkins, Ronan Le Bras, Cristian Petrescu-Prahova, Gabriel Stanovsky, Hannaneh Hajishirzi, and Rik Koncel-Kedziorski</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">SemEval-NAACL</span> North American Chapter of the Association for Computational Linguistics, SemEval-NAACL 2019 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                We report on the SemEval 2019 task on math question answering. We provided a
                question set derived from Math SAT practice exams, including 2778 training questions
                and 1082 test questions. For a significant subset of these questions, we also
                provided SMT-LIB logical form annotations and an interpreter that could solve these
                logical forms. Systems were evaluated based on the percentage of correctly answered
                questions. The top system correctly answered 45% of the test questions, a
                considerable improvement over the 17% random guessing baseline.
            </p>
                                                    </div>
        </div><div class="item mix cpaper AAAI" data-year="2019">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/8209a8703d8c48aaca1523cfa307dd1c069e58f3" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning</h4>
                                                        <div class="pubauthor">Maarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A. Smith, and Yejin Choi</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">AAAI</span> Conference on Artificial Intelligence, AAAI 2019 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                We present ATOMIC, an atlas of everyday commonsense reasoning, organized through
                300k textual descriptions. Compared to existing resources that center around
                taxonomic knowledge, ATOMIC focuses on inferential knowledge organized as typed
                if-then relations with variables (e.g., "if X pays Y a compliment, then Y will
                likely return the compliment"). We propose nine if-then relation types to
                distinguish causes v.s. effects, agents v.s. themes, voluntary v.s. involuntary
                events, and actions v.s. mental states. By generatively training on the rich
                inferential knowledge described in ATOMIC, we show that neural models can acquire
                simple commonsense capabilities and reason about previously unseen events.
                Experimental results demonstrate that multitask models that incorporate the
                hierarchical structure of if-then relation types lead to more accurate inference
                compared to models trained in isolation, as measured by both automatic and human
                evaluation.
            </p>
                                                    </div>
        </div><div class="item mix cpaper EMNLP" data-year="2017">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/Beyond-Sentential-Semantic-Parsing%3A-Tackling-the-a-Hopkins-Petrescu-Prahova/c22a240d1087603664826e9aab809273ed9bff15" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Beyond Sentential Semantic Parsing: Tackling the Math SAT with a Cascade of Tree Transducers</h4>
                                                        <div class="pubauthor">Mark Hopkins, Cristian Petrescu-Prahova, Roie Levin, Ronan Le Bras, Alvaro Herrasti, and Vidur Joshi</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">EMNLP</span> Empirical Methods in Natural Language Processing, EMNLP 2017 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                We present an approach for answering questions that span multiple sentences and
                exhibit sophisticated cross-sentence anaphoric phenomena, evaluating on a rich
                source of such questions – the math portion of the Scholastic Aptitude Test (SAT).
                By using a tree transducer cascade as its basic architecture, our system (called
                EUCLID) propagates uncertainty from multiple sources (e.g. coreference resolution or
                verb interpretation) until it can be confidently resolved. Experiments show the
                first-ever results (43% recall and 91% precision) on SAT algebra word problems. We
                also apply EUCLID to the public Dolphin algebra question set, and improve the
                state-of-the-art F1-score from 73.9% to 77.0%.
            </p>
                                                    </div>
        </div><div class="item mix cpaper IAAI" data-year="2017">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/Phase-Mapper-An-AI-Platform-to-Accelerate-High-Xue-Bai/0147b9fd959c8878ce0c03b91f8c91f9a0a53ef2" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Phase-Mapper: An AI Platform to Accelerate High Throughput Materials Discovery</h4>
                                                        <div class="pubauthor">Yexiang Xue, Junwen Bai, Ronan Le Bras, Brendan Rappazzo, Richard Bernstein, Johan Bjorck, Liane Longpre, Santosh K. Suram, Robert B. van Dover, John Gregoire and Carla P. Gomes</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">IAAI</span> Conference on Innovative Applications of Artificial Intelligence, IAAI 2017 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                High-throughput materials discovery involves the rapid synthesis, measurement, and
                characterization of many different but structurally related materials. A central
                problem in materials discovery, the phase map identification problem, involves the
                determination of the crystal structure of materials from materials composition and
                structural characterization data. We present Phase-Mapper, a novel solution platform
                that allows humans to interact with both the data and products of AI algorithms,
                including the incorporation of human feedback to constrain or initialize solutions.
                Phase-Mapper is compatible with any spectral demixing algorithm, including our novel
                solver, AgileFD, which is based on convolutive non-negative matrix factorization.
                AgileFD allows materials scientists to rapidly interpret XRD patterns, and can
                incorporate constraints to capture the physics of the materials as well as human
                feedback. We compare three solver variants with previously proposed methods in a
                large-scale experiment involving 20 synthetic systems, demonstrating the efficacy of
                imposing physical constraints using AgileFD. Since the deployment of Phase-Mapper at
                the Department of Energy’s Joint Center for Artificial Photosynthesis (JCAP),
                thousands of X-ray diffraction patterns have been processed and the results are
                yielding discovery of new materials for energy applications, as exemplified by the
                discovery of a new family of metal oxide solar light absorbers, among the previously
                unsolved Nb-Mn-V oxide system, which is provided here as an illustrative example.
                Phase-Mapper is also being deployed at the Stanford Synchrotron Radiation
                Lightsource (SSRL) to enable phase mapping on datasets in real time.
            </p>
                                                    </div>
        </div><div class="item mix cpaper CP-AI-OR" data-year="2017">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/2638ac63b4c6ca4ce36a94b3de38a5db56db9f6f" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">In Search of Balance: The Challenge of Generating Balanced Latin Rectangles</h4>
                                                        <div class="pubauthor">Mateo Diaz, Ronan Le Bras and Carla P. Gomes</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">CP-AI-OR</span> International Conference on Integration of Artificial Intelligence and Operations Research Techniques in Constraint Programming, CP-AI-OR 2017 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Spatially Balanced Latin Squares are combinatorial struc- tures of great importance
                for experimental design. From a computational perspective they present a challenging
                problem and there is a need for efficient methods to generate them. Motivated by a
                real-world applica- tion, we consider a natural extension to this problem, balanced
                Latin Rectangles. Balanced Latin Rectangles appear to be even more defiant than
                balanced Latin Squares, to such an extent that perfect balance may not be feasible
                for Latin rectangles. Nonetheless, for real applications, it is still valuable to
                have well balanced Latin rectangles. In this work, we study some of the properties
                of balanced Latin rectangles, prove the nonexistence of perfect balance for an
                infinite family of sizes, and present several methods to generate the most balanced
                solutions.
            </p>
                                                    </div>
        </div><div class="item mix cpaper ACS Combinatorial Science" data-year="2017">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/25d4744e6eaef54c7b00d9de1b7b720588cc59ca" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Automated Phase Mapping with AgileFD and its Application to Light Absorber Discovery in the V–Mn–Nb Oxide System</h4>
                                                        <div class="pubauthor">Santosh K. Suram, Yexiang Xue, Junwen Bai, Ronan Le Bras, Brendan Rappazzo, Richard Bernstein, Johan Bjorck, Lan Zhou, R. Bruce van Dover, Carla P. Gomes and John M. Gregoire</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">ACS Combinatorial Science</span> ACS Combinatorial Science 2017 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Rapid construction of phase diagrams is a central tenet of combinatorial materials
                science with accelerated materials discovery efforts often hampered by challenges in
                interpreting combinatorial X-ray diffraction data sets, which we address by
                developing AgileFD, an artificial intelligence algorithm that enables rapid phase
                mapping from a combinatorial library of X-ray diffraction patterns. AgileFD models
                alloying-based peak shifting through a novel expansion of convolutional nonnegative
                matrix factorization, which not only improves the identification of constituent
                phases but also maps their concentration and lattice parameter as a function of
                composition. By incorporating Gibbs’ phase rule into the algorithm, physically
                meaningful phase maps are obtained with unsupervised operation, and more refined
                solutions are attained by injecting expert knowledge of the system. The algorithm is
                demonstrated through investigation of the V–Mn–Nb oxide system where decomposition
                of eight oxide phases, including two with substantial alloying, provides the first
                phase map for this pseudoternary system. This phase map enables interpretation of
                high-throughput band gap data, leading to the discovery of new solar light absorbers
                and the alloying-based tuning of the direct-allowed band gap energy of MnV2O6. The
                open-source family of AgileFD algorithms can be implemented into a broad range of
                high throughput workflows to accelerate materials discovery.
            </p>
                                                    </div>
        </div><div class="item mix cpaper ICML" data-year="2016">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/Variable-Elimination-in-Fourier-Domain-Xue-Ermon/b992c187ab9bc698d5f6e7bfd3c5d2b85ada84af" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Variable Elimination in the Fourier Domain</h4>
                                                        <div class="pubauthor">Yexiang Xue, Stefano Ermon, Ronan Le Bras, Carla P. Gomes and Bart Selman</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">ICML</span> International Conference on Machine Learning, ICML 2016 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                The ability to represent complex high dimensional probability distributions in a
                compact form is one of the key insights in the field of graphical models. Factored
                representations are ubiquitous in machine learning and lead to major computational
                advantages. We explore a different type of compact representation based on discrete
                Fourier representations, complementing the classical approach based on conditional
                independencies. We show that a large class of probabilistic graphical models have a
                compact Fourier representation. This theoretical result opens up an entirely new way
                of approximating a probability distribution. We demonstrate the significance of this
                approach by applying it to the variable elimination algorithm. Compared with the
                traditional bucket representation and other approximate inference algorithms, we
                obtain significant improvements.
            </p>
                                                    </div>
        </div><div class="item mix jpaper The VLDB Journal" data-year="2015">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/ClouDiA-A-Deployment-Advisor-for-Public-Clouds-Zou-Bras/6d464b7ef845726f1f9071cdf910646ab9d2b0b7" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">ClouDiA: a deployment advisor for public clouds</h4>
                                                        <div class="pubauthor">Tao Zou, Ronan Le Bras, Marcos Vaz Salles, Alan Demers, Johannes Gehrke</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">The VLDB Journal</span> The International Journall on Very Large Data Bases, The VLDB Journal 2015 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                An increasing number of distributed data-driven applications are moving into shared
                public clouds. By sharing resources and operating at scale, public clouds promise
                higher utilization and lower costs than private clusters. To achieve high
                utilization, however, cloud providers inevitably instances of a given application
                may end-up in physically distant machines in the cloud. This allocation strategy can
                lead to large differences in average latency between instances. For a large class of
                applications, this difference can result in significant performance degradation,
                unless care is taken in how application components are mapped to instances. In this
                paper, we propose ClouDiA, a general deployment advisor that selects application
                node deployments minimizing either (i) the largest latency between application
                nodes, or (ii) the longest critical path among all application nodes. ClouDiA
                employs a number of algorithmic techniques, including mixed-integer programming and
                constraint programming techniques, to efficiently search the space of possible
                mappings of application nodes to instances. Through experiments with synthetic and
                real applications in Amazon EC2, we show that mean latency is a robust metric to
                model communication cost in these applications and that our search techniques yield
                a 15–55 % reduction in time-to-solution or service response time, without any need
                for modifying application code.
            </p>
                                                    </div>
        </div><div class="item mix cpaper AAAI" data-year="2015">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/Pattern-Decomposition-with-Complex-Combinatorial-Ermon-Bras/4981ac03e5bf9d009045bc44135cbdb7b34ffbe3" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Pattern Decomposition with Complex Combinatorial Constraints: Application to Materials Discovery</h4>
                                                        <div class="pubauthor">Stefano Ermon, Ronan Le Bras, Santosh Suram, John M. Gregoire, Carla Gomes, Bart Selman and Robert B. van Dover</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">AAAI</span> Conference on Artificial Intelligence, AAAI 2015 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Identifying important components or factors in large amounts of noisy data is a key
                problem in machine learning and data mining. Motivated by a pattern decomposition
                problem in materials discovery, aimed at discovering new materials for renewable
                energy, e.g. for fuel and solar cells, we introduce CombiFD, a framework for factor
                based pattern decomposition that allows the incorporation of a-priori knowledge as
                constraints, including complex combinatorial constraints. In addition , we propose a
                new pattern decomposition algorithm , called AMIQO, based on solving a sequence of
                (mixed-integer) quadratic programs. Our approach considerably outperforms the state
                of the art on the materials discovery problem, scaling to larger datasets and
                recovering more precise and physically meaningful de-compositions. We also show the
                effectiveness of our approach for enforcing background knowledge on other
                application domains.
            </p>
                                                    </div>
        </div><div class="item mix cpaper HCOMP" data-year="2014">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/A-Human-Computation-Framework-for-Boosting-Bras-Xue/83eba5385b5eabbf67047af3e78157170ca0f4f8" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">A Human Computation Framework for Boosting Combinatorial Solvers</h4>
                                                        <div class="pubauthor">Ronan Le Bras, Yexiang Xue, Richard Bernstein, Carla P. Gomes and Bart Selman</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">HCOMP</span> AAAI Conference on Human Computation and Crowdsourcing, HCOMP 2014 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                We propose a general framework for boosting combinatorial solvers through human
                computation. Our framework combines insights from human workers with the power of
                combinatorial optimization. The combinatorial solver is also used to guide requests
                for the workers , and thereby obtain the most useful human feedback quickly. Our
                approach also incorporates a problem decomposition approach with a general strategy
                for discarding incorrect human input. We apply this framework in the domain of
                materials discovery, and demonstrate a speedup of over an order of magnitude.
            </p>
                                                    </div>
        </div><div class="item mix cpaper CP" data-year="2014">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/On-the-Erdos-Discrepancy-Problem-Bras-Gomes/38d1469ccaac0afebc6818dae8408852cc23203f" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">On the Erdos Discrepancy Problem</h4>
                                                        <div class="pubauthor">Ronan Le Bras, Carla P. Gomes, and Bart Selman</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">CP</span> International Conference on Principles and Practice of Constraint Programming, CP 2014 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                According to the Erdős discrepancy conjecture, for any infinite ±1 sequence, there
                exists a homogeneous arithmetic progression of unbounded discrepancy. In other
                words, for any ±1 sequence (x1, x2, ...) and a discrepancy C, there exist integers m
                and d such that | m i=1 x i·d | > C. This is an 80-year-old open problem and
                recent development proved that this conjecture is true for discrepancies up to 2.
                Paul Erd˝ os also conjectured that this property of unbounded discrepancy even holds
                for the restricted case of completely multiplicative sequences (CMSs), namely
                sequences (x1, x2, ...) where x a·b = xa · x b for any a, b ≥ 1. The longest CMS
                with discrepancy 2 has been proven to be of size 246. In this paper, we prove that
                any completely multiplicative sequence of size 127, 646 or more has discrepancy at
                least 4, proving the Erd˝ os discrepancy conjecture for CMSs of discrepancies up to
                3. In addition, we prove that this bound is tight and increases the size of the
                longest known sequence of discrepancy 3 from 17, 000 to 127, 645. Finally, we
                provide inductive construction rules as well as streamlining methods to improve the
                lower bounds for sequences of higher discrepancies.
            </p>
                                                    </div>
        </div><div class="item mix cpaper AAAI" data-year="2014">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/Challenges-in-Materials-Discovery-Synthetic-Bras-Bernstein/d7fec52efd0f0009b73dde5456540373d6e99ff8" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">A Computational Challenge Problem in Materials Discovery: Synthetic Problem Generator and Real-World Datasets</h4>
                                                        <div class="pubauthor">Ronan Le Bras, Richard Bernstein, John M. Gregoire, Santosh K. Suram, Carla P. Gomes, Bart Selman and Robert B. van Dover</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">AAAI</span> Conference on Artificial Intelligence, AAAI 2014 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Newly-discovered materials have been central to recent technological advances. They
                have contributed significantly to breakthroughs in electronics, renewable energy and
                green buildings, and overall, have promoted the advancement of global human welfare.
                Yet, only a fraction of all possible materials have been explored. Accelerating the
                pace of discovery of materials would foster technological innovations, and would
                potentially address pressing issues in sustainability, such as energy production or
                consumption. The bottleneck of this discovery cycle lies, however, in the analysis
                of the materials data. As materials scientists have recently devised techniques to
                efficiently create thousands of materials and experimentalists have developed new
                methods and tools to characterize these materials, the limiting factor has become
                the data analysis itself. Hence, the goal of this paper is to stimulate the
                development of new computational techniques for the analysis of materials data, by
                bringing together the complimentary expertise of materials scientists and computer
                scientists. In collaboration with two major research laboratories in materials
                science, we provide the first publicly available dataset for the phase map
                identification problem. In addition, we provide a parameterized synthetic data
                generator to assess the quality of proposed approaches, as well as tools for data
                visualization and solution evaluation.
            </p>
                                                    </div>
        </div><div class="item mix cpaper IJCAI" data-year="2013">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/Crowdsourcing-Backdoor-Identification-for-Bras-Bernstein/f67920138c6e56eb25a3f5064925626929d14224" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Crowdsourcing Backdoor Identification for Combinatorial Optimization</h4>
                                                        <div class="pubauthor">Ronan Le Bras, Richard Bernstein, Carla P. Gomes, Bart Selman and Robert B. van Dover</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">IJCAI</span> International Joint Conference on Artificial Intelligence, IJCAI 2013 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                We will show how human computation insights can be key to identifying so-called
                backdoor variables in combinatorial optimization problems. Backdoor variables can be
                used to obtain dramatic speed-ups in combinatorial search. Our approach leverages
                the complementary strength of human input, based on a visual identification of
                problem structure , crowdsourcing, and the power of combina-torial solvers to
                exploit complex constraints. We describe our work in the context of the domain of
                materials discovery. The motivation for considering the materials discovery domain
                comes from the fact that new materials can provide solutions for key challenges in
                sustainability, e.g., in energy, new catalysts for more efficient fuel cell
                technology.
            </p>
                                                    </div>
        </div><div class="item mix cpaper IJCAI" data-year="2013">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/Double-Wheel-Graphs-Are-Graceful-Bras-Gomes/7fef8533ebad585fadabdfa24d3a9f5b750df2cb" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Double-Wheel Graphs Are Graceful</h4>
                                                        <div class="pubauthor">Ronan Le Bras, Carla P. Gomes and Bart Selman</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">IJCAI</span> International Joint Conference on Artificial Intelligence, IJCAI 2013 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                We present the first polynomial time construction procedure for generating graceful
                double-wheel graphs. A graph is graceful if its vertices can be labeled with
                distinct integer values from {'{'}0, ..., e{'}'}, where e is the number of edges,
                such that each edge has a unique value corresponding to the absolute difference of
                its endpoints. Graceful graphs have a range of practical application domains,
                including in radio astronomy, X-ray crystallography, cryptography , and experimental
                design. Various families of graphs have been proven to be graceful, while others
                have only been conjectured to be. In particular, it has been conjectured that
                so-called double-wheel graphs are graceful. A double-wheel graph consists of two
                cycles of N nodes connected to a common hub. We prove this conjecture by providing
                the first construction for graceful double-wheel graphs, for any N > 3, using a
                framework that combines streamlined constraint reasoning with insights from human
                computation. We also use this framework to provide a polynomial time construction
                for diagonally ordered magic squares.
            </p>
                                                    </div>
        </div><div class="item mix cpaper VLDB" data-year="2013">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/ClouDiA-A-Deployment-Advisor-for-Public-Clouds-Zou-Bras/6d464b7ef845726f1f9071cdf910646ab9d2b0b7" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">ClouDiA: A Deployment Advisor for Public Clouds</h4>
                                                        <div class="pubauthor">Tao Zou, Ronan Le Bras, Marcos Vaz Salles, Alan Demers and Johannes Gehrke</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">VLDB</span> International Conference on Very Large Data Bases, VLDB 2013 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                An increasing number of distributed data-driven applications are moving into shared
                public clouds. By sharing resources and operating at scale, public clouds promise
                higher utilization and lower costs than private clusters. To achieve high
                utilization, however, cloud providers inevitably allocate virtual machine instances
                non-contiguously; i.e., instances of a given application may end-up in physically
                distant machines in the cloud. This allocation strategy can lead to large
                differences in average latency between instances. For a large class of applications,
                this difference can result in significant performance degradation, unless care is
                taken in how application components are mapped to instances. In this paper, we
                propose ClouDiA, a general deployment advisor that selects application node
                deployments minimizing either (i) the largest latency between application nodes, or
                (ii) the longest critical path among all application nodes. ClouDiA employs a number
                of algorithmic techniques, including mixed-integer programming and constraint
                programming techniques, to efficiently search the space of possible mappings of
                application nodes to instances. Through experiments with synthetic and real
                applications in Amazon EC2, we show that mean latency is a robust metric to model
                communication cost in these applications and that our search techniques yield a
                15–55 % reduction in time-to-solution or service response time, without any need for
                modifying application code.
            </p>
                                                    </div>
        </div><div class="item mix cpaper AAAI" data-year="2013">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/Robust-Network-Design-For-Multispecies-Bras-Dilkina/f5b9fe6e076af2c71ee068a157dc3b43dc763060" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Robust Network Design for Multispecies Conservation</h4>
                                                        <div class="pubauthor">Ronan Le Bras, Bistra Dilkina, Yexiang Xue, Carla P. Gomes, Kevin S. McKelvey, Claire Montgomery and Michael K. Schwartz</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">AAAI</span> Conference on Artificial Intelligence, AAAI 2013 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Our work is motivated by an important network design application in computational
                sustainability concerning wildlife conservation. In the face of human development
                and climate change, it is important that conservation plans for protecting landscape
                connectivity exhibit certain level of robustness. While previous work has focused on
                conservation strategies that result in a connected network of habitat reserves, the
                ro-bustness of the proposed solutions has not been taken into account. In order to
                address this important aspect, we formalize the problem as a node-weighted
                bi-criteria network design problem with connectivity requirements on the number of
                disjoint paths between pairs of nodes. While in most previous work on survivable
                network design the objective is to minimize the cost of the selected network, our
                goal is to optimize the quality of the selected paths within a specified budget ,
                while meeting the connectivity requirements. We characterize the complexity of the
                problem under different restrictions. We provide a mixed-integer programming
                encoding that allows for finding solutions with optimality guarantees, as well as a
                hybrid local search method with better scaling behavior but no guarantees. We
                evaluate the typical-case performance of our approaches using a synthetic benchmark,
                and apply them to a large-scale real-world network design problem concerning the
                conservation of wolverine and lynx populations in the U.S. Rocky Mountains
                (Montana).
            </p>
                                                    </div>
        </div><div class="item mix cpaper AAAI" data-year="2013">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/Large-Landscape-Conservation-Synthetic-and-Real-Dilkina-Lai/a13761ef89916fc815628747596af12331ecaaec" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Large Landscape Conservation - Synthetic and Real-World Datasets</h4>
                                                        <div class="pubauthor">Bistra Dilkina, Katherine Lai, Ronan Le Bras, Yexiang Xue, Carla P. Gomes, Ashish Sabharwal, Jordan Suter, Kevin S. McKelvey, Michael K. Schwartz and Claire Montgomery</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">AAAI</span> Conference on Artificial Intelligence, AAAI 2013 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Biodiversity underpins ecosystem goods and services and hence protecting it is key
                to achieving sustainability. However, the persistence of many species is threatened
                by habitat loss and fragmentation due to human land use and climate change.
                Conservation efforts are implemented under very limited economic resources, and
                therefore designing scalable, cost-efficient and systematic approaches for
                conservation planning is an important and challenging computational task. In
                particular, preserving landscape connectivity between good habitat has become a key
                conservation priority in recent years. We give an overview of landscape connectivity
                conservation and some of the underlying graph-theoretic optimization problems. We
                present a synthetic generator capable of creating families of randomized structured
                problems, capturing the essential features of real-world instances but allowing for
                a thorough typical-case performance evaluation of different solution methods. We
                also present two large-scale real-world datasets, including economic data on land
                cost, and species data for grizzly bears, wolverines and lynx.
            </p>
                                                    </div>
        </div><div class="item mix cpaper SAT" data-year="2013">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/Solutions-for-Hard-and-Soft-Constraints-Using-Finger-Bras/2f520c1d75e51d9650ee4548e0a75d8652c3fd4c" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Solutions for Hard and Soft Constraints Using Optimized Probabilistic Satisfiability</h4>
                                                        <div class="pubauthor">Marcelo Finger, Ronan Le Bras, Carla P. Gomes and Bart Selman</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">SAT</span> International Conference on Theory and Applications of Satisfiability Testing, SAT 2013 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Practical problems often combine real-world hard constraints with soft constraints
                involving preferences, uncertainties or flexible requirements. A probability
                distribution over the models that meet the hard constraints is an answer to such
                problems that is in the spirit of incorporating soft constraints. We propose a
                method using SAT-based reasoning, probabilistic reasoning and linear programming
                that computes such a distribution when soft constraints are interpreted as
                constraints whose violation is bound by a given probability. The method, called
                Optimized Probabilistic Satis-fiability (oPSAT), consists of a two-phase computation
                of a probability distribution over the set of valuations of a SAT formula.
                Algorithms for both phases are presented and their complexity is discussed. We also
                describe an application of the oPSAT technique to the problem of combinatorial
                materials discovery.
            </p>
                                                    </div>
        </div><div class="item mix cpaper AAAI" data-year="2012">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/From-Streamlined-Combinatorial-Search-to-Bras-Gomes/979c32ef783c56555c54f066aa55210160740fed" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">From Streamlined Combinatorial Search to Efficient Constructive Procedures</h4>
                                                        <div class="pubauthor">Ronan Le Bras, Carla P. Gomes and Bart Selman</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">AAAI</span> Conference on Artificial Intelligence, AAAI 2012 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                In recent years, significant progress in the area of search, constraint
                satisfaction, and automated reasoning has been driven in part by the study of
                challenge problems from combinatorics and finite algebra. This work has led to the
                discovery of interesting discrete structures with intricate mathematical properties.
                While some of those results have resolved open questions and conjectures, a
                shortcoming is that they generally do not provide further mathematical insights,
                from which one could derive more general observations. We propose an approach that
                integrates specialized combina-torial search, using so-called streamlining, with a
                human computation component. We use this approach to discover efficient constructive
                procedures for generating certain classes of combinatorial objects of any size. More
                specifically, using our framework, we discovered two complementary efficient
                constructions for generating so-called Spatially Balanced Latin squares (SBLS) of
                any order N, such that 2N+1 is prime. Previously constructions for SBLSs were not
                known. Our approach also enabled us to derive a new lower bound for so-called weak
                Schur numbers, improving on a series of earlier results for Schur numbers.
            </p>
                                                    </div>
        </div><div class="item mix cpaper CompSust" data-year="2012">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/Materials-Discovery-New-Opportunities-at-the-Bras-Ermon/ef69a2b747f0ace01356242574310c19cf0d0ae7" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Materials Discovery: New Opportunities at the Intersection of Constraint Reasoning and Learning</h4>
                                                        <div class="pubauthor">Ronan Le Bras, Stefano Ermon, Theodoros Damoulas, Richard Bernstein, Carla P. Gomes, Bart Selman and Robert B. van Dover</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">CompSust</span> International Conference on Computational Sustainability, CompSust 2012 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Combinatorial materials science involves the rapid, high-throughput synthesis,
                measurement, and analysis of a large number of different but structurally related
                materials. In combinatorial materials discovery, materials scientists search for
                intermetallic compounds with desirable physical properties by obtaining measurements
                on hundreds of samples from a composition spread. Determining the structure of the
                materials formed in a composition spread is key to understanding composition and
                property relations and can potentially result in a breakthrough materials discovery.
                This is an important and exciting direction in the emerging field of computational
                sustainability [4] as it aims to achieve the best possible use of our available
                material resources. One ultimate objective is to help discover the next-generation
                materials for fuel-cell catalysis, as such materials have the potential of
                dramatically increasing fuel cell capacity while reducing their cost. The analysis
                of composition spreads remains, however, a manual and laborious task. Thus the need
                for new techniques to automatically analyze and interpret such data. Whereas the
                data-intensive aspect of the area of materials discovery seems to favor Data-Mining
                or Machine Learning techniques, the rigorous and highly-structured physical
                properties that govern the crystallization on the composition spread interestingly
                suggest that constraint reasoning is key to a physically meaningful analysis. In
                this paper, we describe two novel approaches to this problem that integrate
                domain-specific scientific background knowledge about the physical and chemical
                properties of the materials. Our first approach combines constraint programming (CP)
                and machine learning (ML), while the second is based on satisfiability modulo theory
                (SMT). We evaluate the performance of our methods on realistic synthetic
                measurements, and we show that it provides accurate and physically meaningful
                interpretations of the data, even in the presence of artificially added noise.
            </p>
                                                    </div>
        </div><div class="item mix cpaper SAT" data-year="2012">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/SMT-Aided-Combinatorial-Materials-Discovery-Ermon-Bras/b2b7ac83ac7a1abe4e335288089e739406660cfa" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">SMT-Aided Combinatorial Materials Discovery</h4>
                                                        <div class="pubauthor">Stefano Ermon, Ronan Le Bras, Carla P. Gomes, Bart Selman and Robert B. van Dover</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">SAT</span> International Conference on Theory and Applications of Satisfiability Testing, SAT 2012 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                In combinatorial materials discovery, one searches for new materials with desirable
                properties by obtaining measurements on hundreds of samples in a single
                high-throughput batch experiment. As manual data analysis is becoming more and more
                impractical, there is a growing need to develop new techniques to automatically
                analyze and interpret such data. We describe a novel approach to the phase map
                identification problem where we integrate domain-specific scientific background
                knowledge about the physical and chemical properties of the materials into an SMT
                reasoning framework. We evaluate the performance of our method on realistic
                synthetic measurements, and we show that it provides accurate and physically
                meaningful interpretations of the data, even in the presence of artificially added
                noise.
            </p>
                                                    </div>
        </div><div class="item mix cpaper CP" data-year="2011">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/Constraint-Reasoning-and-Kernel-Clustering-for-Bras-Damoulas/320fa73c66c48c380756ca1ca2c8a08af5a077ad" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Constraint Reasoning and Kernel Clustering for Pattern Decomposition With Scaling</h4>
                                                        <div class="pubauthor">Ronan Le Bras, Theodoros Damoulas, John M. Gregoire, Ashish Sabharwal, Carla P. Gomes and Robert B. van Dover</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">CP</span> International Conference on Principles and Practice of Constraint Programming, CP 2011 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Motivated by an important and challenging task encountered in material discovery, we
                consider the problem of finding K basis patterns of numbers that jointly compose N
                observed patterns while enforcing additional spatial and scaling constraints. We
                propose a Constraint Programming (CP) model which captures the exact problem
                structure yet fails to scale in the presence of noisy data about the patterns. We
                alleviate this issue by employing Machine Learning (ML) techniques, namely kernel
                methods and clustering, to decompose the problem into smaller ones based on a global
                data-driven view, and then stitch the partial solutions together using a global CP
                model. Combining the complementary strengths of CP and ML techniques yields a more
                accurate and scalable method than the few found in the literature for this complex
                problem.
            </p>
                                                    </div>
        </div><div class="item mix cpaper CP" data-year="2009">
                                                    <div class="pubmain">
                                                        <div class="pubassets">

                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://www.semanticscholar.org/paper/Efficient-Generic-Search-Heuristics-within-the-Bras-Zanarini/8a7a1969005f873a7803d3ff0cae4a3d27072c90" title="Download" target="_blank">

                                                                <i class="fa fa-external-link"></i>
                                                            </a>
                                                        </div>

                                                        <h4 class="pubtitle">Efficient Generic Search Heuristics within the EMBP Framework</h4>
                                                        <div class="pubauthor">Ronan Le Bras, Alessandro Zanarini and Gilles Pesant</div>
                                                        <div class="pubauthor">
                                                        </div>
                                                        <div class="pubcite"><span class="label label-success">CP</span> International Conference on Principles and Practice of Constraint Programming, CP 2009 </div>

                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>
                Accurately estimating the distribution of solutions to a problem , should such
                solutions exist, provides efficient search heuristics. The purpose of this paper is
                to propose new ways of computing such estimates , with different degrees of accuracy
                and complexity. We build on the Expectation-Maximization Belief-Propagation (EMPB)
                framework proposed by Hsu et al. to solve Constraint Satisfaction Problems (CSPs).
                We propose two general approaches within the EMBP framework: we firstly derive
                update rules at the constraint level while enforcing domain consistency and then
                derive update rules globally, at the problem level. The contribution of this paper
                is twofold: first, we derive new generic update rules suited to tackle any CSP;
                second, we propose an efficient EMBP-inspired approach, thereby improving this
                method and making it competitive with the state of the art. We evaluate these
                approaches experimentally and demonstrate their effectiveness.
            </p>
                                                    </div>
        </div>