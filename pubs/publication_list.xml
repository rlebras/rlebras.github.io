    <ul>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/NeuroLogic-Decoding%3A-(Un)supervised-Neural-Text-Lu-West/2c5bf29079cd958a2bef150077a02a1deb300652">
                    NeuroLogic Decoding: (Un)supervised Neural Text Generation with Predicate Logic Constraints
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Ximing Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi
                </strong>
            </BodySmall>
            <BodySmall>NAACL | 2021</BodySmall>
            <p>
                Conditional text generation often requires lexical constraints, i.e., which words should or shouldn't be included in the output text. While the dominant recipe for conditional text generation has been large-scale pretrained language models that are finetuned on the task-specific training data, such models do not learn to follow the underlying constraints reliably, even when supervised with large amounts of task-specific examples. We propose NeuroLogic Decoding, a simple yet effective algorithm that enables neural language models -- supervised or not -- to generate fluent text while satisfying complex lexical constraints. Our approach is powerful yet efficient. It handles any set of lexical constraints that is expressible under predicate logic, while its asymptotic runtime is equivalent to conventional beam search. Empirical results on four benchmarks show that NeuroLogic Decoding outperforms previous approaches, including algorithms that handle a subset of our constraints. Moreover, we find that unsupervised models with NeuroLogic Decoding often outperform supervised models with conventional decoding, even when the latter is based on considerably larger networks. Our results suggest the limit of large-scale neural networks for fine-grained controllable generation and the promise of inference-time algorithms.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://api.semanticscholar.org/e39503e01ebb108c6773948a24ca798cd444eb62">
                    COMET-ATOMIC 2020: On Symbolic and Neural Commonsense Knowledge Graphs
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke Sakaguchi, Antoine Bosselut, Yejin Choi
                </strong>
            </BodySmall>
            <BodySmall>AAAI | 2021</BodySmall>
            <p>
                Recent years have brought about a renewed interest in commonsense representation and reasoning in the field of natural language understanding. The development of new commonsense knowledge graphs (CSKG) has been central to these advances as their diverse facts can be used and referenced by machine learning models for tackling new and challenging tasks. At the same time, there remain questions about the quality and coverage of these resources due to the massive scale required to comprehensively encompass general commonsense knowledge.
In this work, we posit that manually constructed CSKGs will never achieve the coverage necessary to be applicable in all situations encountered by NLP agents. Therefore, we propose a new evaluation framework for testing the utility of KGs based on how effectively implicit knowledge representations can be learned from them.
With this new goal, we propose ATOMIC 2020, a new CSKG of general-purpose commonsense knowledge containing knowledge that is not readily available in pretrained language models. We evaluate its properties in comparison with other leading CSKGs, performing the first large-scale pairwise study of commonsense knowledge resources. Next, we show that ATOMIC 2020 is better suited for training knowledge models that can generate accurate, representative knowledge for new, unseen entities and events. Finally, through human evaluation, we show that the few-shot performance of GPT-3 (175B parameters), while impressive, remains ~12 absolute points lower than a BART-based knowledge model trained on ATOMIC 2020 despite using over 430x fewer parameters.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="">
                    UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New Multitask Benchmark
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Nicholas Lourie, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi
                </strong>
            </BodySmall>
            <BodySmall>AAAI | 2021</BodySmall>
            <p>
                As AI systems become an increasing part of people's everyday lives, it becomes ever more important that they understand people's ethical norms. Motivated by descriptive ethics, a field of study that focuses on people's descriptive judgments rather than theoretical prescriptions on morality, we investigate a novel, data-driven approach to machine ethics.
We introduce SCRUPLES, the first large-scale dataset with 625,000 ethical judgments over 32,000 real-life anecdotes. Each anecdote recounts a complex ethical situation, often posing moral dilemmas, paired with a distribution of judgments contributed by the community members. Our dataset presents a major challenge to state-of-the-art neural language models, leaving significant room for improvement. However, when presented with simplified moral situations, the results are considerably more promising, suggesting that neural models can effectively learn simpler ethical building blocks.
A key take-away of our empirical analysis is that norms are not always clean-cut; many situations are naturally divisive. We present a new method to estimate the best possible performance on such tasks with inherently diverse label distributions, and explore likelihood functions that separate intrinsic from model uncertainty.
                   </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://api.semanticscholar.org/ba3c0aa5c9057140e08872e908efe48791af3083">
                    Scruples: A Corpus of Community Ethical Judgments on 32, 000 Real-Life Anecdotes
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Nicholas Lourie, Ronan Le Bras, Yejin Choi
                </strong>
            </BodySmall>
            <BodySmall>AAAI | 2021</BodySmall>
            <p>
                As AI systems become an increasing part of people's everyday lives, it becomes ever more important that they understand people's ethical norms. Motivated by descriptive ethics, a field of study that focuses on people's descriptive judgments rather than theoretical prescriptions on morality, we investigate a novel, data-driven approach to machine ethics.
We introduce Scruples, the first large-scale dataset with 625,000 ethical judgments over 32,000 real-life anecdotes. Each anecdote recounts a complex ethical situation, often posing moral dilemmas, paired with a distribution of judgments contributed by the community members. Our dataset presents a major challenge to state-of-the-art neural language models, leaving significant room for improvement. However, when presented with simplified moral situations, the results are considerably more promising, suggesting that neural models can effectively learn simpler ethical building blocks.
A key take-away of our empirical analysis is that norms are not always clean-cut; many situations are naturally divisive. We present a new method to estimate the best possible performance on such tasks with inherently diverse label distributions, and explore likelihood functions that separate intrinsic from model uncertainty.
            </p>
        </Result.Item>




        <Result.Item border="none">
            <Result.Title>
                <a href="https://api.semanticscholar.org/e2ecce8134a736444065e28a5c12344245b13f7d">
                    Dynamic Neuro-Symbolic Knowledge Graph Construction for Zero-shot Commonsense Question Answering
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Antoine Bosselut, Ronan Le Bras, Yejin Choi
                </strong>
            </BodySmall>
            <BodySmall>AAAI | 2021</BodySmall>
            <p>
                Understanding narratives requires reasoning about implicit world knowledge related to the causes, effects, and states of situations described in text. At the core of this challenge is how to access contextually relevant knowledge on demand and reason over it. In this paper, we present initial studies toward zero-shot commonsense question answering by formulating the task as inference over dynamically generated commonsense knowledge graphs. In contrast to previous studies for knowledge integration that rely on retrieval of existing knowledge from static knowledge graphs, our study requires commonsense knowledge integration where contextually relevant knowledge is often not present in existing knowledge bases. Therefore, we present a novel approach that generates contextually-relevant symbolic knowledge structures on demand using generative neural commonsense knowledge models. Empirical results on two datasets demonstrate the efficacy of our neuro-symbolic approach for dynamically constructing knowledge graphs for reasoning. Our approach achieves significant performance boosts over pretrained language models and vanilla knowledge models, all while providing interpretable reasoning paths for its predictions.
            </p>
        </Result.Item>


        <Result.Item border="none">
            <Result.Title>
                <a href="https://api.semanticscholar.org/5dfc43bb697acf5eacf8b8a05d78dba8beb0dd42">
                    Paragraph-Level Commonsense Transformers with Recurrent Memory
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Saadia Gabriel, Chandra Bhagavatula, Vered Shwartz, Ronan Le Bras, Max Forbes, Yejin Choi
                </strong>
            </BodySmall>
            <BodySmall>AAAI | 2021</BodySmall>
            <p>
                Human understanding of narrative texts requires making commonsense inferences beyond what is stated in the text explicitly. A recent model, COMeT, can generate such inferences along several dimensions such as pre- and post-conditions, motivations, and mental-states of the participants. However, COMeT was trained on short phrases, and is therefore discourse-agnostic. When presented with each sentence of a multi-sentence narrative, it might generate inferences that are inconsistent with the rest of the narrative.
We present the task of discourse-aware commonsense inference. Given a sentence within a narrative, the goal is to generate commonsense inferences along predefined dimensions, while maintaining coherence with the rest of the narrative. Such large-scale paragraph-level annotation is hard to get and costly, so we use available sentence-level annotations to efficiently and automatically construct a distantly supervised corpus.
Using this corpus, we train PARA-COMeT, a discourse-aware model that incorporates paragraph-level information to generate coherent commonsense inferences from narratives. PARA-COMeT captures both semantic knowledge pertaining to prior world knowledge, and episodic knowledge involving how current events relate to prior and future events in a narrative. Our results confirm that PARA-COMeT outperforms the sentence-level baselines, particularly in generating inferences that are both coherent and novel.
            </p>
        </Result.Item>



        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/Unsupervised-Commonsense-Question-Answering-with-Shwartz-West/a45b430f057a48b2d4c31c9278248c2b43780bf8">
                    Unsupervised Commonsense Question Answering with Self-Talk
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Vered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi
                </strong>
            </BodySmall>
            <BodySmall>EMNLP | 2020</BodySmall>
            <p>
                Natural language understanding involves reading between the lines with implicit background knowledge. Current systems either rely on pre-trained language models as the sole implicit source of world knowledge, or resort to external knowledge bases (KBs) to incorporate additional relevant knowledge. We propose an unsupervised framework based on \emph{self-talk} as a novel alternative to multiple-choice commonsense tasks. Inspired by inquiry-based discovery learning (Bruner, 1961), our approach inquires language models with a number of information seeking questions such as "$\textit{what is the definition of ...}$" to discover additional background knowledge. Empirical results demonstrate that the self-talk procedure substantially improves the performance of zero-shot language model baselines on four out of six commonsense benchmarks, and competes with models that obtain knowledge from external KBs. While our approach improves performance on several benchmarks, the self-talk induced knowledge even when leading to correct answers is not always seen as useful by human judges, raising interesting questions about the inner-workings of pre-trained language models for commonsense reasoning.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/Back-to-the-Future%3A-Unsupervised-Backprop-based-for-Qin-Shwartz/14c454c27dffd655cea839a0684a2d855117cd58">
                    Back to the Future: Unsupervised Backprop-based Decoding for Counterfactual and Abductive Commonsense Reasoning
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Lianhui Qin, Vered Shwartz, P. West, Chandra Bhagavatula, Jena D. Hwang, Ronan Le Bras, Antoine Bosselut, Yejin Choi
                </strong>
            </BodySmall>
            <BodySmall>EMNLP | 2020</BodySmall>
            <p>
               Abductive and counterfactual reasoning, core abilities of everyday human cognition, require reasoning about what might have happened at time t, while conditioning on multiple contexts from the relative past and future. However, simultaneous incorporation of past and future contexts using generative language models (LMs) can be challenging, as they are trained either to condition only on the past context or to perform narrowly scoped text-infilling. In this paper, we propose DELOREAN, a new unsupervised decoding algorithm that can flexibly incorporate both the past and future contexts using only off-the-shelf, left-to-right language models and no supervision. The key intuition of our algorithm is incorporating the future through back-propagation, during which, we only update the internal representation of the output while fixing the model parameters. By alternating between forward and backward propagation, DELOREAN can decode the output representation that reflects both the left and right contexts. We demonstrate that our approach is general and applicable to two nonmonotonic reasoning tasks: abductive text generation and counterfactual story revision, where DELOREAN outperforms a range of unsupervised and some supervised methods, based on automatic and human evaluation.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/Thinking-Like-a-Skeptic%3A-Defeasible-Inference-in-Rudinger-Shwartz/47e799f83b0850f3d036a2e3a66bb337661b7e68">
                    Thinking Like a Skeptic: Defeasible Inference in Natural Language
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Rachel Rudinger, Vered Shwartz, Jena D. Hwang, Chandra Bhagavatula, Maxwell Forbes, Ronan Le Bras, Noah A. Smith, Yejin Choi
                </strong>
            </BodySmall>
            <BodySmall>Findings of EMNLP | 2020</BodySmall>
            <p>
                Defeasible inference is a mode of reasoning in which an inference (X is a bird, therefore X flies) may be weakened or overturned in light of new evidence (X is a penguin). Though long recognized in classical AI and philosophy, defeasible inference has not been extensively studied in the context of contemporary data-driven research on natural language inference and commonsense reasoning. We introduce Defeasible NLI (abbreviated δ-NLI), a dataset for defeasible inference in natural language. δ-NLI contains extensions to three existing inference datasets covering diverse modes of reasoning: common sense, natural language inference, and social norms. From δ-NLI, we develop both a classification and generation task for defeasible inference, and demonstrate that the generation task is much more challenging. Despite lagging human performance, however, generative models trained on this data are capable of writing sentences that weaken or strengthen a specified inference up to 68% of the time.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/Natural-Language-Rationales-with-Full-Stack-Visual-Marasovi%C4%87-Bhagavatula/c9940a17504a3b83bd1e9d613b095ddb204d2ad0">
                    Natural Language Rationales with Full-Stack Visual Reasoning: From Pixels to Semantic Frames to Commonsense Graphs
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Ana Marasović, Chandra Bhagavatula, J. Park, Ronan Le Bras, Noah A. Smith, Yejin Choi
                </strong>
            </BodySmall>
            <BodySmall>Findings of EMNLP | 2020</BodySmall>
            <p>
                Natural language rationales could provide intuitive, higher-level explanations that are easily understandable by humans, complementing the more broadly studied lower-level explanations based on gradients or attention weights. We present the first study focused on generating natural language rationales across several complex visual reasoning tasks: visual commonsense reasoning, visual-textual entailment, and visual question answering. The key challenge of accurate rationalization is comprehensive image understanding at all levels: not just their explicit content at the pixel level, but their contextual contents at the semantic and pragmatic levels. We present RATIONALEVT TRANSFORMER, an integrated model that learns to generate free-text rationales by combining pretrained language models with object recognition, grounded visual semantic frames, and visual commonsense graphs. Our experiments show that the base pretrained language model benefits from visual adaptation and that freetext rationalization is a promising research direction to complement model interpretability for complex visual-textual reasoning tasks.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/Generative-Data-Augmentation-for-Commonsense-Yang-Malaviya/dd6f3b6d92ae9448a2000d9690b921f545f00256">
                   Generative Data Augmentation for Commonsense Reasoning
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan Le Bras, J. Wang, Chandra Bhagavatula, Yejin Choi, Doug Downey
                </strong>
            </BodySmall>
            <BodySmall>Findings of EMNLP | 2020</BodySmall>
            <p>
                Recent advances in commonsense reasoning depend on large-scale human-annotated training data to achieve peak performance. However, manual curation of training examples is expensive and has been shown to introduce annotation artifacts that neural models can readily exploit and overfit on. We investigate G-DAUG, a novel generative data augmentation method that aims to achieve more accurate and robust learning in the low-resource setting. Our approach generates synthetic examples using pretrained language models, and selects the most informative and diverse set of examples for data augmentation. In experiments with multiple commonsense reasoning benchmarks, G-DAUG consistently outperforms existing data augmentation methods based on back-translation, and establishes a new state-of-the-art on WinoGrande, CODAH, and CommonsenseQA. Further, in addition to improvements in in-distribution accuracy, G-DAUG-augmented training also enhances out-of-distribution generalization, showing greater robustness against adversarial or perturbed examples. Our analysis demonstrates that G-DAUG produces a diverse set of fluent training examples, and that its selection and training approaches are important for performance. Our findings encourage future research toward generative data augmentation to enhance both in-distribution learning and out-of-distribution generalization.
            </p>
        </Result.Item>


        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/Adversarial-Filters-of-Dataset-Biases-Bras-Swayamdipta/22d834f7983fbd7cf2418978571f23efcd224bd9">
                    Adversarial Filters of Dataset Biases
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Ronan Le Bras, Swabha Swayamdipta, Chandra Bhagavatula, Rowan Zellers, Matthew E. Peters, Ashish Sabharwal, Yejin Choi
                </strong>
            </BodySmall>
            <BodySmall>ICML | 2020</BodySmall>
            <p>
                Large neural models have demonstrated human-level performance on language and vision benchmarks such as ImageNet and Stanford Natural Language Inference (SNLI). Yet, their performance degrades considerably when tested on adversarial or out-of-distribution samples. This raises the question of whether these models have learned to solve a dataset rather than the underlying task by overfitting on spurious dataset biases. We investigate one recently proposed approach, AFLite, which adversarially filters such dataset biases, as a means to mitigate the prevalent overestimation of machine performance. We provide a theoretical understanding for AFLite, by situating it in the generalized framework for optimum bias reduction. Our experiments show that as a result of the substantial reduction of these biases, models trained on the filtered datasets yield better generalization to out-of-distribution tasks, especially when the benchmarks used for training are over-populated with biased samples. We show that AFLite is broadly applicable to a variety of both real and synthetic datasets for reduction of measurable dataset biases and provide extensive supporting analyses. Finally, filtering results in a large drop in model performance (e.g., from 92% to 63% for SNLI), while human performance still remains high. Our work thus shows that such filtered datasets can pose new research challenges for robust generalization by serving as upgraded benchmarks.
            </p>
        </Result.Item>

                <Result.Item border="none">
            <Result.Title>
                <a href="https://www.dropbox.com/s/wziu46961t34crn/Jensenetal_RR_022020.pdf?dl=0">
                    Conspicuous Monitoring and Remote Work
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Nathaniel Jensen, Elizabeth Lyons, Eddy Chebelyon, Ronan Le Bras, Carla Gomes
                </strong>
            </BodySmall>
            <BodySmall>J Econ Behav Organ. | 2020</BodySmall>
            <p>
                Credible monitoring of remote workers presents unique challenges that may reduce the benefits
of formal organization for their management. We consider whether increasing the salience of
monitor productivity without changing incentive contracts or monitoring technology leads to
changes in remote worker performance. Results from a field experiment run among multidimensional task workers in Kenya demonstrate that increasing the visibility of monitor activity
improves performance on task dimensions not being directly paid for. Our evidence is consistent
with the importance of conspicuous monitoring when managers and workers are not co-located.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/a550f576ff20b8cce98f3ddad0043d3783fbc9b4">
                    Abductive Commonsense Reasoning
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari
                    Holtzman, Hannah Rashkin, Doug Downey, Scott Yih, Yejin Choi
                </strong>
            </BodySmall>
            <BodySmall>ICLR | 2020</BodySmall>
            <p>
                Abductive reasoning is inference to the most plausible explanation. For example, if
                Jenny finds her house in a mess when she returns from work, and remembers that she
                left a window open, she can hypothesize that a thief broke into her house and caused
                the mess, as the most plausible explanation. While abduction has long been
                considered to be at the core of how people interpret and read between the lines in
                natural language (Hobbs et al. (1988)), there has been relatively little NLP
                research in support of abductive natural language inference. We present the first
                study that investigates the viability of language-based abductive reasoning. We
                conceptualize a new task of Abductive NLI and introduce a challenge dataset, ART,
                that consists of over 20k commonsense narrative contexts and 200k explanations,
                formulated as multiple choice questions for easy automatic evaluation. We establish
                comprehensive baseline performance on this task based on state-of-the-art NLI and
                language models, which leads to 68.9% accuracy, well below human performance
                (91.4%). Our analysis leads to new insights into the types of reasoning that deep
                pre-trained language models fail to perform -- despite their strong performance on
                the related but fundamentally different task of entailment NLI -- pointing to
                interesting avenues for future research.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/8f7133b2e3851b09d659b91e8faa761ec206413f">
                    WinoGrande: An Adversarial Winograd Schema Challenge at Scale
                </a>
            </Result.Title>
            <BodySmall>
                <strong>Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi</strong>
            </BodySmall>
            <BodySmall>AAAI | 2020</BodySmall>
            <p>
                The Winograd Schema Challenge (WSC), proposed by Levesque et al. (2011) as an
                alternative to the Turing Test, was originally designed as a pronoun resolution
                problem that cannot be solved based on statistical patterns in large text corpora.
                However, recent studies suggest that current WSC datasets, even when composed
                carefully by experts, are still prone to such biases that statistical methods can
                exploit. We introduce WINOGRANDE, a new collection of WSC problems that are
                adversarially constructed to be robust against spurious statistical biases. While
                the original WSC dataset provided only 273 instances, WINOGRANDE includes 43,985
                instances, half of which are determined as adversarial. Key to our approach is a
                novel adversarial filtering algorithm AFLITE for systematic bias reduction, combined
                with a careful crowdsourcing design. Despite the significant increase in training
                data, the performance of existing state-of-the-art methods remains modest (61.6%)
                and contrasts with high human performance (90.8%) for the binary questions. In
                addition, WINOGRANDE allows us to use transfer learning for achieving new
                state-of-the-art results on the original WSC and related datasets. Finally, we
                discuss how biases lead to overestimating the true capabilities of machine
                commonsense.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/04f4e55e14150b7c48b0287ba77c7443df76ed45">
                    PIQA: Reasoning about Physical Commonsense in Natural Language
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, Yejin Choi
                </strong>
            </BodySmall>
            <BodySmall>AAAI | 2020</BodySmall>
            <p>
                To apply eyeshadow without a brush, should I use a cotton swab or a toothpick?
                Questions requiring this kind of physical commonsense pose a challenge to today’s
                natural language understanding systems. While recent pretrained models (such as
                BERT) have made progress on question answering over more abstract domains – such as
                news articles and encyclopedia entries, where text is plentiful – in more physical
                domains, text is inherently limited due to reporting bias. Can AI systems learn to
                reliably answer physical commonsense questions without experiencing the physical
                world? In this paper, we introduce the task of physical commonsense reasoning and a
                corresponding benchmark dataset Physical Interaction: Question Answering or PIQA .
                Though humans find the dataset easy (95% accuracy), large pretrained models struggle
                (∼77%). We provide analysis about the dimensions of knowledge that existing models
                lack, which offers significant opportunities for future research.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/421cb75cc91e8e5683d41ee6a918121aedf6d24d">
                    Social IQa: Commonsense Reasoning about Social Interactions
                </a>
            </Result.Title>
            <BodySmall>
                <strong>Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, Yejin Choi</strong>
            </BodySmall>
            <BodySmall>EMNLP | 2019</BodySmall>
            <p>
                We introduce Social IQa, the first largescale benchmark for commonsense reasoning
                about social situations. Social IQa contains 38,000 multiple choice questions for
                probing emotional and social intelligence in a variety of everyday situations (e.g.,
                Q: "Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did
                Jordan do this?" A: "Make sure no one else could hear"). Through crowdsourcing, we
                collect commonsense questions along with correct and incorrect answers about social
                interactions, using a new framework that mitigates stylistic artifacts in incorrect
                answers by asking workers to provide the right answer to a different but related
                question. Empirical results show that our benchmark is challenging for existing
                question-answering models based on pretrained language models, compared to human
                performance (&gt;20% gap). Notably, we further establish Social IQa as a resource
                for transfer learning of commonsense knowledge, achieving state-of-the-art
                performance on multiple commonsense reasoning tasks (Winograd Schemas, COPA).
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://api.semanticscholar.org/arXiv:1909.00277">
                    Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning
                </a>
            </Result.Title>
            <BodySmall>
                <strong>Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi</strong>
            </BodySmall>
            <BodySmall>EMNLP | 2019</BodySmall>
            <p>
                Understanding narratives requires reading between the lines, which in turn, requires
                interpreting the likely causes and effects of events, even when they are not
                mentioned explicitly. In this paper, we introduce Cosmos QA, a large-scale dataset
                of 35,600 problems that require commonsense-based reading comprehension, formulated
                as multiple-choice questions. In stark contrast to most existing reading
                comprehension datasets where the questions focus on factual and literal
                understanding of the context paragraph, our dataset focuses on reading between the
                lines over a diverse collection of people's everyday narratives, asking such
                questions as "what might be the possible reason of ...?", or "what would have
                happened if ..." that require reasoning beyond the exact text spans in the context.
                To establish baseline performances on Cosmos QA, we experiment with several
                state-of-the-art neural architectures for reading comprehension, and also propose a
                new architecture that improves over the competitive baselines. Experimental results
                demonstrate a significant gap between machine (68.4%) and human performance (94%),
                pointing to avenues for future research on commonsense machine comprehension.
                Dataset, code and leaderboard is publicly available at this
                <a href="https://wilburone.github.io/cosmos"> https URL</a>.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/655acd2a4dd59f541e6542594139e1b332543305">
                    SemEval 2019 Task 10: Math Question Answering
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Mark Hopkins, Ronan Le Bras, Cristian Petrescu-Prahova, Gabriel Stanovsky,
                    Hannaneh Hajishirzi, and Rik Koncel-Kedziorski
                </strong>
            </BodySmall>
            <BodySmall>SemEval-NAACL | 2019</BodySmall>
            <p>
                We report on the SemEval 2019 task on math question answering. We provided a
                question set derived from Math SAT practice exams, including 2778 training questions
                and 1082 test questions. For a significant subset of these questions, we also
                provided SMT-LIB logical form annotations and an interpreter that could solve these
                logical forms. Systems were evaluated based on the percentage of correctly answered
                questions. The top system correctly answered 45% of the test questions, a
                considerable improvement over the 17% random guessing baseline.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/8209a8703d8c48aaca1523cfa307dd1c069e58f3">
                    ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Maarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie,
                    Hannah Rashkin, Brendan Roof, Noah A. Smith, and Yejin Choi
                </strong>
            </BodySmall>
            <BodySmall>AAAI | 2019</BodySmall>
            <p>
                We present ATOMIC, an atlas of everyday commonsense reasoning, organized through
                300k textual descriptions. Compared to existing resources that center around
                taxonomic knowledge, ATOMIC focuses on inferential knowledge organized as typed
                if-then relations with variables (e.g., "if X pays Y a compliment, then Y will
                likely return the compliment"). We propose nine if-then relation types to
                distinguish causes v.s. effects, agents v.s. themes, voluntary v.s. involuntary
                events, and actions v.s. mental states. By generatively training on the rich
                inferential knowledge described in ATOMIC, we show that neural models can acquire
                simple commonsense capabilities and reason about previously unseen events.
                Experimental results demonstrate that multitask models that incorporate the
                hierarchical structure of if-then relation types lead to more accurate inference
                compared to models trained in isolation, as measured by both automatic and human
                evaluation.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/Beyond-Sentential-Semantic-Parsing%3A-Tackling-the-a-Hopkins-Petrescu-Prahova/c22a240d1087603664826e9aab809273ed9bff15">
                    Beyond Sentential Semantic Parsing: Tackling the Math SAT with a Cascade of Tree
                    Transducers
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Mark Hopkins, Cristian Petrescu-Prahova, Roie Levin, Ronan Le Bras, Alvaro
                    Herrasti, and Vidur Joshi
                </strong>
            </BodySmall>
            <BodySmall>EMNLP | 2017</BodySmall>
            <p>
                We present an approach for answering questions that span multiple sentences and
                exhibit sophisticated cross-sentence anaphoric phenomena, evaluating on a rich
                source of such questions – the math portion of the Scholastic Aptitude Test (SAT).
                By using a tree transducer cascade as its basic architecture, our system (called
                EUCLID) propagates uncertainty from multiple sources (e.g. coreference resolution or
                verb interpretation) until it can be confidently resolved. Experiments show the
                first-ever results (43% recall and 91% precision) on SAT algebra word problems. We
                also apply EUCLID to the public Dolphin algebra question set, and improve the
                state-of-the-art F1-score from 73.9% to 77.0%.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/Phase-Mapper-An-AI-Platform-to-Accelerate-High-Xue-Bai/0147b9fd959c8878ce0c03b91f8c91f9a0a53ef2">
                    Phase-Mapper: An AI Platform to Accelerate High Throughput Materials Discovery
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Yexiang Xue, Junwen Bai, Ronan Le Bras, Brendan Rappazzo, Richard Bernstein,
                    Johan Bjorck, Liane Longpre, Santosh K. Suram, Robert B. van Dover, John
                    Gregoire and Carla P. Gomes
                </strong>
            </BodySmall>
            <BodySmall>IAAI | 2017 | Best paper award</BodySmall>
            <p>
                High-throughput materials discovery involves the rapid synthesis, measurement, and
                characterization of many different but structurally related materials. A central
                problem in materials discovery, the phase map identification problem, involves the
                determination of the crystal structure of materials from materials composition and
                structural characterization data. We present Phase-Mapper, a novel solution platform
                that allows humans to interact with both the data and products of AI algorithms,
                including the incorporation of human feedback to constrain or initialize solutions.
                Phase-Mapper is compatible with any spectral demixing algorithm, including our novel
                solver, AgileFD, which is based on convolutive non-negative matrix factorization.
                AgileFD allows materials scientists to rapidly interpret XRD patterns, and can
                incorporate constraints to capture the physics of the materials as well as human
                feedback. We compare three solver variants with previously proposed methods in a
                large-scale experiment involving 20 synthetic systems, demonstrating the efficacy of
                imposing physical constraints using AgileFD. Since the deployment of Phase-Mapper at
                the Department of Energy’s Joint Center for Artificial Photosynthesis (JCAP),
                thousands of X-ray diffraction patterns have been processed and the results are
                yielding discovery of new materials for energy applications, as exemplified by the
                discovery of a new family of metal oxide solar light absorbers, among the previously
                unsolved Nb-Mn-V oxide system, which is provided here as an illustrative example.
                Phase-Mapper is also being deployed at the Stanford Synchrotron Radiation
                Lightsource (SSRL) to enable phase mapping on datasets in real time.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/2638ac63b4c6ca4ce36a94b3de38a5db56db9f6f">
                    In Search of Balance: The Challenge of Generating Balanced Latin Rectangles
                </a>
            </Result.Title>
            <BodySmall>
                <strong>Mateo Diaz, Ronan Le Bras and Carla P. Gomes</strong>
            </BodySmall>
            <BodySmall>CP-AI-OR | 2017</BodySmall>
            <p>
                Spatially Balanced Latin Squares are combinatorial struc- tures of great importance
                for experimental design. From a computational perspective they present a challenging
                problem and there is a need for efficient methods to generate them. Motivated by a
                real-world applica- tion, we consider a natural extension to this problem, balanced
                Latin Rectangles. Balanced Latin Rectangles appear to be even more defiant than
                balanced Latin Squares, to such an extent that perfect balance may not be feasible
                for Latin rectangles. Nonetheless, for real applications, it is still valuable to
                have well balanced Latin rectangles. In this work, we study some of the properties
                of balanced Latin rectangles, prove the nonexistence of perfect balance for an
                infinite family of sizes, and present several methods to generate the most balanced
                solutions.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/25d4744e6eaef54c7b00d9de1b7b720588cc59ca">
                    Automated Phase Mapping with AgileFD and its Application to Light Absorber
                    Discovery in the V–Mn–Nb Oxide System
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Santosh K. Suram, Yexiang Xue, Junwen Bai, Ronan Le Bras, Brendan Rappazzo,
                    Richard Bernstein, Johan Bjorck, Lan Zhou, R. Bruce van Dover, Carla P. Gomes
                    and John M. Gregoire
                </strong>
            </BodySmall>
            <BodySmall>ACS Combinatorial Science | 2017</BodySmall>
            <p>
                Rapid construction of phase diagrams is a central tenet of combinatorial materials
                science with accelerated materials discovery efforts often hampered by challenges in
                interpreting combinatorial X-ray diffraction data sets, which we address by
                developing AgileFD, an artificial intelligence algorithm that enables rapid phase
                mapping from a combinatorial library of X-ray diffraction patterns. AgileFD models
                alloying-based peak shifting through a novel expansion of convolutional nonnegative
                matrix factorization, which not only improves the identification of constituent
                phases but also maps their concentration and lattice parameter as a function of
                composition. By incorporating Gibbs’ phase rule into the algorithm, physically
                meaningful phase maps are obtained with unsupervised operation, and more refined
                solutions are attained by injecting expert knowledge of the system. The algorithm is
                demonstrated through investigation of the V–Mn–Nb oxide system where decomposition
                of eight oxide phases, including two with substantial alloying, provides the first
                phase map for this pseudoternary system. This phase map enables interpretation of
                high-throughput band gap data, leading to the discovery of new solar light absorbers
                and the alloying-based tuning of the direct-allowed band gap energy of MnV2O6. The
                open-source family of AgileFD algorithms can be implemented into a broad range of
                high throughput workflows to accelerate materials discovery.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/Variable-Elimination-in-Fourier-Domain-Xue-Ermon/b992c187ab9bc698d5f6e7bfd3c5d2b85ada84af">
                    Variable Elimination in the Fourier Domain
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Yexiang Xue, Stefano Ermon, Ronan Le Bras, Carla P. Gomes and Bart Selman
                </strong>
            </BodySmall>
            <BodySmall>ICML | 2016</BodySmall>
            <p>
                The ability to represent complex high dimensional probability distributions in a
                compact form is one of the key insights in the field of graphical models. Factored
                representations are ubiquitous in machine learning and lead to major computational
                advantages. We explore a different type of compact representation based on discrete
                Fourier representations, complementing the classical approach based on conditional
                independencies. We show that a large class of probabilistic graphical models have a
                compact Fourier representation. This theoretical result opens up an entirely new way
                of approximating a probability distribution. We demonstrate the significance of this
                approach by applying it to the variable elimination algorithm. Compared with the
                traditional bucket representation and other approximate inference algorithms, we
                obtain significant improvements.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/ClouDiA-A-Deployment-Advisor-for-Public-Clouds-Zou-Bras/6d464b7ef845726f1f9071cdf910646ab9d2b0b7">
                    ClouDiA: a deployment advisor for public clouds
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Tao Zou, Ronan Le Bras, Marcos Vaz Salles, Alan Demers, Johannes Gehrke
                </strong>
            </BodySmall>
            <BodySmall>The VLDB Journal | 2015</BodySmall>
            <p>
                An increasing number of distributed data-driven applications are moving into shared
                public clouds. By sharing resources and operating at scale, public clouds promise
                higher utilization and lower costs than private clusters. To achieve high
                utilization, however, cloud providers inevitably instances of a given application
                may end-up in physically distant machines in the cloud. This allocation strategy can
                lead to large differences in average latency between instances. For a large class of
                applications, this difference can result in significant performance degradation,
                unless care is taken in how application components are mapped to instances. In this
                paper, we propose ClouDiA, a general deployment advisor that selects application
                node deployments minimizing either (i) the largest latency between application
                nodes, or (ii) the longest critical path among all application nodes. ClouDiA
                employs a number of algorithmic techniques, including mixed-integer programming and
                constraint programming techniques, to efficiently search the space of possible
                mappings of application nodes to instances. Through experiments with synthetic and
                real applications in Amazon EC2, we show that mean latency is a robust metric to
                model communication cost in these applications and that our search techniques yield
                a 15–55 % reduction in time-to-solution or service response time, without any need
                for modifying application code.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/Pattern-Decomposition-with-Complex-Combinatorial-Ermon-Bras/4981ac03e5bf9d009045bc44135cbdb7b34ffbe3">
                    Pattern Decomposition with Complex Combinatorial Constraints: Application to
                    Materials Discovery
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Stefano Ermon, Ronan Le Bras, Santosh Suram, John M. Gregoire, Carla Gomes, Bart
                    Selman and Robert B. van Dover
                </strong>
            </BodySmall>
            <BodySmall>AAAI | 2015</BodySmall>
            <p>
                Identifying important components or factors in large amounts of noisy data is a key
                problem in machine learning and data mining. Motivated by a pattern decomposition
                problem in materials discovery, aimed at discovering new materials for renewable
                energy, e.g. for fuel and solar cells, we introduce CombiFD, a framework for factor
                based pattern decomposition that allows the incorporation of a-priori knowledge as
                constraints, including complex combinatorial constraints. In addition , we propose a
                new pattern decomposition algorithm , called AMIQO, based on solving a sequence of
                (mixed-integer) quadratic programs. Our approach considerably outperforms the state
                of the art on the materials discovery problem, scaling to larger datasets and
                recovering more precise and physically meaningful de-compositions. We also show the
                effectiveness of our approach for enforcing background knowledge on other
                application domains.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/A-Human-Computation-Framework-for-Boosting-Bras-Xue/83eba5385b5eabbf67047af3e78157170ca0f4f8">
                    A Human Computation Framework for Boosting Combinatorial Solvers
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Ronan Le Bras, Yexiang Xue, Richard Bernstein, Carla P. Gomes and Bart Selman
                </strong>
            </BodySmall>
            <BodySmall>HCOMP | 2014</BodySmall>
            <p>
                We propose a general framework for boosting combinatorial solvers through human
                computation. Our framework combines insights from human workers with the power of
                combinatorial optimization. The combinatorial solver is also used to guide requests
                for the workers , and thereby obtain the most useful human feedback quickly. Our
                approach also incorporates a problem decomposition approach with a general strategy
                for discarding incorrect human input. We apply this framework in the domain of
                materials discovery, and demonstrate a speedup of over an order of magnitude.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/On-the-Erdos-Discrepancy-Problem-Bras-Gomes/38d1469ccaac0afebc6818dae8408852cc23203f">
                    On the Erdos Discrepancy Problem
                </a>
            </Result.Title>
            <BodySmall>
                <strong>Ronan Le Bras, Carla P. Gomes, and Bart Selman</strong>
            </BodySmall>
            <BodySmall>CP | 2014</BodySmall>
            <p>
                According to the Erdős discrepancy conjecture, for any infinite ±1 sequence, there
                exists a homogeneous arithmetic progression of unbounded discrepancy. In other
                words, for any ±1 sequence (x1, x2, ...) and a discrepancy C, there exist integers m
                and d such that | m i=1 x i·d | &gt; C. This is an 80-year-old open problem and
                recent development proved that this conjecture is true for discrepancies up to 2.
                Paul Erd˝ os also conjectured that this property of unbounded discrepancy even holds
                for the restricted case of completely multiplicative sequences (CMSs), namely
                sequences (x1, x2, ...) where x a·b = xa · x b for any a, b ≥ 1. The longest CMS
                with discrepancy 2 has been proven to be of size 246. In this paper, we prove that
                any completely multiplicative sequence of size 127, 646 or more has discrepancy at
                least 4, proving the Erd˝ os discrepancy conjecture for CMSs of discrepancies up to
                3. In addition, we prove that this bound is tight and increases the size of the
                longest known sequence of discrepancy 3 from 17, 000 to 127, 645. Finally, we
                provide inductive construction rules as well as streamlining methods to improve the
                lower bounds for sequences of higher discrepancies.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/Challenges-in-Materials-Discovery-Synthetic-Bras-Bernstein/d7fec52efd0f0009b73dde5456540373d6e99ff8">
                    A Computational Challenge Problem in Materials Discovery: Synthetic Problem
                    Generator and Real-World Datasets
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Ronan Le Bras, Richard Bernstein, John M. Gregoire, Santosh K. Suram, Carla P.
                    Gomes, Bart Selman and Robert B. van Dover
                </strong>
            </BodySmall>
            <BodySmall>AAAI | 2014</BodySmall>
            <p>
                Newly-discovered materials have been central to recent technological advances. They
                have contributed significantly to breakthroughs in electronics, renewable energy and
                green buildings, and overall, have promoted the advancement of global human welfare.
                Yet, only a fraction of all possible materials have been explored. Accelerating the
                pace of discovery of materials would foster technological innovations, and would
                potentially address pressing issues in sustainability, such as energy production or
                consumption. The bottleneck of this discovery cycle lies, however, in the analysis
                of the materials data. As materials scientists have recently devised techniques to
                efficiently create thousands of materials and experimentalists have developed new
                methods and tools to characterize these materials, the limiting factor has become
                the data analysis itself. Hence, the goal of this paper is to stimulate the
                development of new computational techniques for the analysis of materials data, by
                bringing together the complimentary expertise of materials scientists and computer
                scientists. In collaboration with two major research laboratories in materials
                science, we provide the first publicly available dataset for the phase map
                identification problem. In addition, we provide a parameterized synthetic data
                generator to assess the quality of proposed approaches, as well as tools for data
                visualization and solution evaluation.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/Crowdsourcing-Backdoor-Identification-for-Bras-Bernstein/f67920138c6e56eb25a3f5064925626929d14224">
                    Crowdsourcing Backdoor Identification for Combinatorial Optimization
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Ronan Le Bras, Richard Bernstein, Carla P. Gomes, Bart Selman and Robert B. van
                    Dover
                </strong>
            </BodySmall>
            <BodySmall>IJCAI | 2013</BodySmall>
            <p>
                We will show how human computation insights can be key to identifying so-called
                backdoor variables in combinatorial optimization problems. Backdoor variables can be
                used to obtain dramatic speed-ups in combinatorial search. Our approach leverages
                the complementary strength of human input, based on a visual identification of
                problem structure , crowdsourcing, and the power of combina-torial solvers to
                exploit complex constraints. We describe our work in the context of the domain of
                materials discovery. The motivation for considering the materials discovery domain
                comes from the fact that new materials can provide solutions for key challenges in
                sustainability, e.g., in energy, new catalysts for more efficient fuel cell
                technology.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/Double-Wheel-Graphs-Are-Graceful-Bras-Gomes/7fef8533ebad585fadabdfa24d3a9f5b750df2cb">
                    Double-Wheel Graphs Are Graceful
                </a>
            </Result.Title>
            <BodySmall>
                <strong>Ronan Le Bras, Carla P. Gomes and Bart Selman</strong>
            </BodySmall>
            <BodySmall>IJCAI | 2013</BodySmall>
            <p>
                We present the first polynomial time construction procedure for generating graceful
                double-wheel graphs. A graph is graceful if its vertices can be labeled with
                distinct integer values from {'{'}0, ..., e{'}'}, where e is the number of edges,
                such that each edge has a unique value corresponding to the absolute difference of
                its endpoints. Graceful graphs have a range of practical application domains,
                including in radio astronomy, X-ray crystallography, cryptography , and experimental
                design. Various families of graphs have been proven to be graceful, while others
                have only been conjectured to be. In particular, it has been conjectured that
                so-called double-wheel graphs are graceful. A double-wheel graph consists of two
                cycles of N nodes connected to a common hub. We prove this conjecture by providing
                the first construction for graceful double-wheel graphs, for any N &gt; 3, using a
                framework that combines streamlined constraint reasoning with insights from human
                computation. We also use this framework to provide a polynomial time construction
                for diagonally ordered magic squares.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/ClouDiA-A-Deployment-Advisor-for-Public-Clouds-Zou-Bras/6d464b7ef845726f1f9071cdf910646ab9d2b0b7">
                    ClouDiA: A Deployment Advisor for Public Clouds
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Tao Zou, Ronan Le Bras, Marcos Vaz Salles, Alan Demers and Johannes Gehrke
                </strong>
            </BodySmall>
            <BodySmall>VLDB | 2013</BodySmall>
            <p>
                An increasing number of distributed data-driven applications are moving into shared
                public clouds. By sharing resources and operating at scale, public clouds promise
                higher utilization and lower costs than private clusters. To achieve high
                utilization, however, cloud providers inevitably allocate virtual machine instances
                non-contiguously; i.e., instances of a given application may end-up in physically
                distant machines in the cloud. This allocation strategy can lead to large
                differences in average latency between instances. For a large class of applications,
                this difference can result in significant performance degradation, unless care is
                taken in how application components are mapped to instances. In this paper, we
                propose ClouDiA, a general deployment advisor that selects application node
                deployments minimizing either (i) the largest latency between application nodes, or
                (ii) the longest critical path among all application nodes. ClouDiA employs a number
                of algorithmic techniques, including mixed-integer programming and constraint
                programming techniques, to efficiently search the space of possible mappings of
                application nodes to instances. Through experiments with synthetic and real
                applications in Amazon EC2, we show that mean latency is a robust metric to model
                communication cost in these applications and that our search techniques yield a
                15–55 % reduction in time-to-solution or service response time, without any need for
                modifying application code.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/Robust-Network-Design-For-Multispecies-Bras-Dilkina/f5b9fe6e076af2c71ee068a157dc3b43dc763060">
                    Robust Network Design for Multispecies Conservation
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Ronan Le Bras, Bistra Dilkina, Yexiang Xue, Carla P. Gomes, Kevin S. McKelvey,
                    Claire Montgomery and Michael K. Schwartz
                </strong>
            </BodySmall>
            <BodySmall>AAAI | 2013</BodySmall>
            <p>
                Our work is motivated by an important network design application in computational
                sustainability concerning wildlife conservation. In the face of human development
                and climate change, it is important that conservation plans for protecting landscape
                connectivity exhibit certain level of robustness. While previous work has focused on
                conservation strategies that result in a connected network of habitat reserves, the
                ro-bustness of the proposed solutions has not been taken into account. In order to
                address this important aspect, we formalize the problem as a node-weighted
                bi-criteria network design problem with connectivity requirements on the number of
                disjoint paths between pairs of nodes. While in most previous work on survivable
                network design the objective is to minimize the cost of the selected network, our
                goal is to optimize the quality of the selected paths within a specified budget ,
                while meeting the connectivity requirements. We characterize the complexity of the
                problem under different restrictions. We provide a mixed-integer programming
                encoding that allows for finding solutions with optimality guarantees, as well as a
                hybrid local search method with better scaling behavior but no guarantees. We
                evaluate the typical-case performance of our approaches using a synthetic benchmark,
                and apply them to a large-scale real-world network design problem concerning the
                conservation of wolverine and lynx populations in the U.S. Rocky Mountains
                (Montana).
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/Large-Landscape-Conservation-Synthetic-and-Real-Dilkina-Lai/a13761ef89916fc815628747596af12331ecaaec">
                    Large Landscape Conservation - Synthetic and Real-World Datasets
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Bistra Dilkina, Katherine Lai, Ronan Le Bras, Yexiang Xue, Carla P. Gomes,
                    Ashish Sabharwal, Jordan Suter, Kevin S. McKelvey, Michael K. Schwartz and
                    Claire Montgomery
                </strong>
            </BodySmall>
            <BodySmall>AAAI | 2013</BodySmall>
            <p>
                Biodiversity underpins ecosystem goods and services and hence protecting it is key
                to achieving sustainability. However, the persistence of many species is threatened
                by habitat loss and fragmentation due to human land use and climate change.
                Conservation efforts are implemented under very limited economic resources, and
                therefore designing scalable, cost-efficient and systematic approaches for
                conservation planning is an important and challenging computational task. In
                particular, preserving landscape connectivity between good habitat has become a key
                conservation priority in recent years. We give an overview of landscape connectivity
                conservation and some of the underlying graph-theoretic optimization problems. We
                present a synthetic generator capable of creating families of randomized structured
                problems, capturing the essential features of real-world instances but allowing for
                a thorough typical-case performance evaluation of different solution methods. We
                also present two large-scale real-world datasets, including economic data on land
                cost, and species data for grizzly bears, wolverines and lynx.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/Solutions-for-Hard-and-Soft-Constraints-Using-Finger-Bras/2f520c1d75e51d9650ee4548e0a75d8652c3fd4c">
                    Solutions for Hard and Soft Constraints Using Optimized Probabilistic
                    Satisfiability
                </a>
            </Result.Title>
            <BodySmall>
                <strong>Marcelo Finger, Ronan Le Bras, Carla P. Gomes and Bart Selman</strong>
            </BodySmall>
            <BodySmall>SAT | 2013</BodySmall>
            <p>
                Practical problems often combine real-world hard constraints with soft constraints
                involving preferences, uncertainties or flexible requirements. A probability
                distribution over the models that meet the hard constraints is an answer to such
                problems that is in the spirit of incorporating soft constraints. We propose a
                method using SAT-based reasoning, probabilistic reasoning and linear programming
                that computes such a distribution when soft constraints are interpreted as
                constraints whose violation is bound by a given probability. The method, called
                Optimized Probabilistic Satis-fiability (oPSAT), consists of a two-phase computation
                of a probability distribution over the set of valuations of a SAT formula.
                Algorithms for both phases are presented and their complexity is discussed. We also
                describe an application of the oPSAT technique to the problem of combinatorial
                materials discovery.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/From-Streamlined-Combinatorial-Search-to-Bras-Gomes/979c32ef783c56555c54f066aa55210160740fed">
                    From Streamlined Combinatorial Search to Efficient Constructive Procedures
                </a>
            </Result.Title>
            <BodySmall>
                <strong>Ronan Le Bras, Carla P. Gomes and Bart Selman</strong>
            </BodySmall>
            <BodySmall>AAAI | 2012</BodySmall>
            <p>
                In recent years, significant progress in the area of search, constraint
                satisfaction, and automated reasoning has been driven in part by the study of
                challenge problems from combinatorics and finite algebra. This work has led to the
                discovery of interesting discrete structures with intricate mathematical properties.
                While some of those results have resolved open questions and conjectures, a
                shortcoming is that they generally do not provide further mathematical insights,
                from which one could derive more general observations. We propose an approach that
                integrates specialized combina-torial search, using so-called streamlining, with a
                human computation component. We use this approach to discover efficient constructive
                procedures for generating certain classes of combinatorial objects of any size. More
                specifically, using our framework, we discovered two complementary efficient
                constructions for generating so-called Spatially Balanced Latin squares (SBLS) of
                any order N, such that 2N+1 is prime. Previously constructions for SBLSs were not
                known. Our approach also enabled us to derive a new lower bound for so-called weak
                Schur numbers, improving on a series of earlier results for Schur numbers.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/Materials-Discovery-New-Opportunities-at-the-Bras-Ermon/ef69a2b747f0ace01356242574310c19cf0d0ae7">
                    Materials Discovery: New Opportunities at the Intersection of Constraint
                    Reasoning and Learning
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Ronan Le Bras, Stefano Ermon, Theodoros Damoulas, Richard Bernstein, Carla P.
                    Gomes, Bart Selman and Robert B. van Dover
                </strong>
            </BodySmall>
            <BodySmall>CompSust | 2012</BodySmall>
            <p>
                Combinatorial materials science involves the rapid, high-throughput synthesis,
                measurement, and analysis of a large number of different but structurally related
                materials. In combinatorial materials discovery, materials scientists search for
                intermetallic compounds with desirable physical properties by obtaining measurements
                on hundreds of samples from a composition spread. Determining the structure of the
                materials formed in a composition spread is key to understanding composition and
                property relations and can potentially result in a breakthrough materials discovery.
                This is an important and exciting direction in the emerging field of computational
                sustainability [4] as it aims to achieve the best possible use of our available
                material resources. One ultimate objective is to help discover the next-generation
                materials for fuel-cell catalysis, as such materials have the potential of
                dramatically increasing fuel cell capacity while reducing their cost. The analysis
                of composition spreads remains, however, a manual and laborious task. Thus the need
                for new techniques to automatically analyze and interpret such data. Whereas the
                data-intensive aspect of the area of materials discovery seems to favor Data-Mining
                or Machine Learning techniques, the rigorous and highly-structured physical
                properties that govern the crystallization on the composition spread interestingly
                suggest that constraint reasoning is key to a physically meaningful analysis. In
                this paper, we describe two novel approaches to this problem that integrate
                domain-specific scientific background knowledge about the physical and chemical
                properties of the materials. Our first approach combines constraint programming (CP)
                and machine learning (ML), while the second is based on satisfiability modulo theory
                (SMT). We evaluate the performance of our methods on realistic synthetic
                measurements, and we show that it provides accurate and physically meaningful
                interpretations of the data, even in the presence of artificially added noise.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/SMT-Aided-Combinatorial-Materials-Discovery-Ermon-Bras/b2b7ac83ac7a1abe4e335288089e739406660cfa">
                    SMT-Aided Combinatorial Materials Discovery
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Stefano Ermon, Ronan Le Bras, Carla P. Gomes, Bart Selman and Robert B. van
                    Dover
                </strong>
            </BodySmall>
            <BodySmall>SAT | 2012</BodySmall>
            <p>
                In combinatorial materials discovery, one searches for new materials with desirable
                properties by obtaining measurements on hundreds of samples in a single
                high-throughput batch experiment. As manual data analysis is becoming more and more
                impractical, there is a growing need to develop new techniques to automatically
                analyze and interpret such data. We describe a novel approach to the phase map
                identification problem where we integrate domain-specific scientific background
                knowledge about the physical and chemical properties of the materials into an SMT
                reasoning framework. We evaluate the performance of our method on realistic
                synthetic measurements, and we show that it provides accurate and physically
                meaningful interpretations of the data, even in the presence of artificially added
                noise.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/Constraint-Reasoning-and-Kernel-Clustering-for-Bras-Damoulas/320fa73c66c48c380756ca1ca2c8a08af5a077ad">
                    Constraint Reasoning and Kernel Clustering for Pattern Decomposition With
                    Scaling
                </a>
            </Result.Title>
            <BodySmall>
                <strong>
                    Ronan Le Bras, Theodoros Damoulas, John M. Gregoire, Ashish Sabharwal, Carla P.
                    Gomes and Robert B. van Dover
                </strong>
            </BodySmall>
            <BodySmall>CP | 2011</BodySmall>
            <p>
                Motivated by an important and challenging task encountered in material discovery, we
                consider the problem of finding K basis patterns of numbers that jointly compose N
                observed patterns while enforcing additional spatial and scaling constraints. We
                propose a Constraint Programming (CP) model which captures the exact problem
                structure yet fails to scale in the presence of noisy data about the patterns. We
                alleviate this issue by employing Machine Learning (ML) techniques, namely kernel
                methods and clustering, to decompose the problem into smaller ones based on a global
                data-driven view, and then stitch the partial solutions together using a global CP
                model. Combining the complementary strengths of CP and ML techniques yields a more
                accurate and scalable method than the few found in the literature for this complex
                problem.
            </p>
        </Result.Item>

        <Result.Item border="none">
            <Result.Title>
                <a href="https://www.semanticscholar.org/paper/Efficient-Generic-Search-Heuristics-within-the-Bras-Zanarini/8a7a1969005f873a7803d3ff0cae4a3d27072c90">
                    Efficient Generic Search Heuristics within the EMBP Framework
                </a>
            </Result.Title>
            <BodySmall>
                <strong>Ronan Le Bras, Alessandro Zanarini and Gilles Pesant</strong>
            </BodySmall>
            <BodySmall>CP | 2009</BodySmall>
            <p>
                Accurately estimating the distribution of solutions to a problem , should such
                solutions exist, provides efficient search heuristics. The purpose of this paper is
                to propose new ways of computing such estimates , with different degrees of accuracy
                and complexity. We build on the Expectation-Maximization Belief-Propagation (EMPB)
                framework proposed by Hsu et al. to solve Constraint Satisfaction Problems (CSPs).
                We propose two general approaches within the EMBP framework: we firstly derive
                update rules at the constraint level while enforcing domain consistency and then
                derive update rules globally, at the problem level. The contribution of this paper
                is twofold: first, we derive new generic update rules suited to tackle any CSP;
                second, we propose an efficient EMBP-inspired approach, thereby improving this
                method and making it competitive with the state of the art. We evaluate these
                approaches experimentally and demonstrate their effectiveness.
            </p>
        </Result.Item>
    </ul>
